---
title: "10.13 ç½‘ç»œçˆ¬è™«å¼€å‘æŒ‡å—"
description: "æ·±å…¥è§£æTRQuantç½‘ç»œçˆ¬è™«å¼€å‘ï¼ŒåŒ…æ‹¬çˆ¬è™«æŠ€æœ¯åŸºç¡€ã€æŠ€æœ¯æ ˆé€‰æ‹©ã€çˆ¬è™«å®ç°ã€åçˆ¬ç­–ç•¥åº”å¯¹ã€æ•°æ®æ”¶é›†å·¥å…·ç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œä¸ºæ•°æ®æ”¶é›†å·¥å…·å¼€å‘æä¾›å®Œæ•´çš„å¼€å‘æŒ‡å¯¼"
lang: "zh-CN"
layout: "/src/layouts/HandbookLayout.astro"
currentBook: "ashare-book6"
updateDate: "2025-12-12"
---

# ğŸ•·ï¸ 10.13 ç½‘ç»œçˆ¬è™«å¼€å‘æŒ‡å—

> **æ ¸å¿ƒæ‘˜è¦ï¼š**
> 
> æœ¬èŠ‚ç³»ç»Ÿä»‹ç»TRQuantç½‘ç»œçˆ¬è™«å¼€å‘ï¼ŒåŒ…æ‹¬çˆ¬è™«æŠ€æœ¯åŸºç¡€ã€æŠ€æœ¯æ ˆé€‰æ‹©ã€çˆ¬è™«å®ç°ã€åçˆ¬ç­–ç•¥åº”å¯¹ã€æ•°æ®æ”¶é›†å·¥å…·ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡ç†è§£ç½‘ç»œçˆ¬è™«å¼€å‘çš„å®Œæ•´æ–¹æ³•ï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡æ•°æ®æ”¶é›†å·¥å…·çš„å¼€å‘æŠ€å·§ï¼Œä¸ºæ„å»ºçŸ¥è¯†åº“å’Œæ”¶é›†æ•°æ®å¥ å®šåŸºç¡€ã€‚

ç½‘ç»œçˆ¬è™«æ˜¯æ•°æ®æ”¶é›†çš„é‡è¦å·¥å…·ï¼Œç”¨äºä»äº’è”ç½‘è·å–ä¿¡æ¯å¹¶æ„å»ºçŸ¥è¯†åº“ã€‚TRQuantç³»ç»Ÿæ•´åˆäº†å¤šä¸ªå¼€æºçˆ¬è™«å·¥å…·ï¼Œæä¾›å®Œæ•´çš„æ•°æ®æ”¶é›†è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç« èŠ‚æ¦‚è§ˆ

<script>
function scrollToSection(sectionId) {
  const element = document.getElementById(sectionId);
  if (element) {
    const headerOffset = 100;
    const elementPosition = element.getBoundingClientRect().top;
    const offsetPosition = elementPosition + window.pageYOffset - headerOffset;
    window.scrollTo({
      top: offsetPosition,
      behavior: 'smooth'
    });
  }
}
</script>

<div class="section-overview">
  <div class="section-item" onclick="scrollToSection('section-10-13-1')">
    <h4>ğŸ”¬ 10.13.1 çˆ¬è™«æŠ€æœ¯åŸºç¡€</h4>
    <p>çˆ¬è™«æ¦‚è¿°ã€å·¥ä½œæµç¨‹ã€çˆ¬è™«ç±»å‹ã€æŠ€æœ¯åŸç†</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-13-2')">
    <h4>ğŸ› ï¸ 10.13.2 æŠ€æœ¯æ ˆé€‰æ‹©</h4>
    <p>å·¥å…·å¯¹æ¯”ã€é€‰æ‹©å»ºè®®ã€æŠ€æœ¯æ ˆæ•´åˆ</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-13-3')">
    <h4>ğŸ’» 10.13.3 çˆ¬è™«å®ç°</h4>
    <p>åŸºç¡€çˆ¬è™«ã€Scrapyçˆ¬è™«ã€Playwrightçˆ¬è™«ã€APIçˆ¬è™«</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-13-4')">
    <h4>ğŸ›¡ï¸ 10.13.4 åçˆ¬ç­–ç•¥åº”å¯¹</h4>
    <p>å¸¸è§åçˆ¬æªæ–½ã€åº”å¯¹ç­–ç•¥ã€robots.txtéµå®ˆ</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-13-5')">
    <h4>ğŸ“¦ 10.13.5 æ•°æ®æ”¶é›†å·¥å…·</h4>
    <p>WebCrawlerã€PDFDownloaderã€AcademicScraperã€SourceRecommender</p>
  </div>
</div>

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- **ç†è§£çˆ¬è™«æŠ€æœ¯**ï¼šæŒæ¡çˆ¬è™«æŠ€æœ¯åŸºç¡€å’Œå·¥ä½œæµç¨‹
- **é€‰æ‹©æŠ€æœ¯æ ˆ**ï¼šæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„çˆ¬è™«å·¥å…·
- **å®ç°çˆ¬è™«**ï¼šæŒæ¡åŸºç¡€çˆ¬è™«ã€Scrapyã€Playwrightç­‰å®ç°æ–¹æ³•
- **åº”å¯¹åçˆ¬**ï¼šæŒæ¡å¸¸è§åçˆ¬ç­–ç•¥çš„åº”å¯¹æ–¹æ³•
- **ä½¿ç”¨å·¥å…·**ï¼šæŒæ¡TRQuantæ•°æ®æ”¶é›†å·¥å…·çš„ä½¿ç”¨æ–¹æ³•

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### çˆ¬è™«ç±»å‹

- **é™æ€ç½‘é¡µçˆ¬è™«**ï¼šé€‚ç”¨äºä¼ ç»ŸHTMLé¡µé¢ï¼ˆrequests + Beautiful Soupï¼‰
- **åŠ¨æ€ç½‘é¡µçˆ¬è™«**ï¼šé€‚ç”¨äºJavaScriptæ¸²æŸ“çš„é¡µé¢ï¼ˆPlaywrightï¼‰
- **APIçˆ¬è™«**ï¼šé€‚ç”¨äºRESTful APIï¼ˆrequestsï¼‰

### æŠ€æœ¯æ ˆ

- **requests + Beautiful Soup**ï¼šç®€å•é™æ€ç½‘é¡µ
- **Scrapy**ï¼šå¤§è§„æ¨¡çˆ¬å–
- **Playwright**ï¼šåŠ¨æ€ç½‘é¡µæ¸²æŸ“
- **arXiv API**ï¼šå­¦æœ¯è®ºæ–‡ä¸‹è½½

<h2 id="section-10-13-1">ğŸ”¬ 10.13.1 çˆ¬è™«æŠ€æœ¯åŸºç¡€</h2>

ç½‘ç»œçˆ¬è™«ï¼ˆWeb Crawlerï¼‰æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–ç¨‹åºï¼Œç”¨äºä»ç½‘é¡µæå–ä¿¡æ¯ã€ä¸‹è½½å†…å®¹ã€ç›‘æ§æ›´æ–°ç­‰ã€‚

### çˆ¬è™«æ¦‚è¿°

ç½‘ç»œçˆ¬è™«çš„ä¸»è¦ç”¨é€”ï¼š

- **æ•°æ®æ”¶é›†**ï¼šä»ç½‘é¡µæå–ä¿¡æ¯
- **å†…å®¹æŠ“å–**ï¼šä¸‹è½½æ–‡æ¡£ã€å›¾ç‰‡ã€è§†é¢‘ç­‰
- **ä¿¡æ¯ç›‘æ§**ï¼šå®šæœŸæ£€æŸ¥ç½‘ç«™æ›´æ–°
- **çŸ¥è¯†åº“æ„å»º**ï¼šæ”¶é›†æ•°æ®ç”¨äºçŸ¥è¯†åº“

### å·¥ä½œæµç¨‹

```
èµ·å§‹URL
    â†“
å‘é€è¯·æ±‚ (HTTP)
    â†“
è§£æå“åº” (HTML/JSON)
    â†“
æå–æ•°æ® (Content)
    â†“
å‘ç°é“¾æ¥ (Links)
    â†“
ä¿å­˜æ•°æ® (Storage)
    â†“
ç»§ç»­çˆ¬å– (Recurse)
```

### çˆ¬è™«ç±»å‹

#### é™æ€ç½‘é¡µçˆ¬è™«

é€‚ç”¨äºä¼ ç»ŸHTMLé¡µé¢ï¼š

<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_static_web_crawler.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
import requests
from bs4 import Beautifu<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_dynamic_web_crawler.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()
    page.goto("https://example.com")
    content = page.content()
    browser.close()
```
-->sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()
    page.goto("https://example.com")
    content = page.content()
    browser.close()
```

<h2 id="section-10-13-2">ğŸ› ï¸ 10.13.2 æŠ€æœ¯æ ˆé€‰æ‹©</h2>

TRQuantç³»ç»Ÿæ•´åˆäº†å¤šä¸ªå¼€æºçˆ¬è™«å·¥å…·ï¼Œæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„å·¥å…·ã€‚

### å·¥å…·å¯¹æ¯”

| å·¥å…· | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|---------|------|------|
| **requests + Beautiful Soup** | ç®€å•é™æ€ç½‘é¡µ | è½»é‡ã€æ˜“ç”¨ | ä¸æ”¯æŒJavaScript |
| **Scrapy** | å¤§è§„æ¨¡çˆ¬å– | é«˜æ€§èƒ½ã€<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13___init__.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# extension/python/tools/data_collector/web_crawler.py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import random
from typing import List, Optional, Dict
from pathlib import Path
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class WebCrawler:
    """ç½‘é¡µçˆ¬è™« - åŸºäº requests + Beautiful Soup"""
    
    def __init__(self, output_dir: Path, 
                 delay_range: tuple = (1, 3),
                 respect_robots: bool = True,
                 proxy_config: Optional[Dict] = None,
                 max_retries: int = 3):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.delay_range = delay_range
        self.respect_robots = respect_robots
        self.max_retries = max_retries
        
        # åˆ›å»ºsession
        self.session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®User-Agent
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def collect(self, url: str, max_depth: int = 2,
                allowed_domains: Optional[List[str]] = None,
                allowed_patterns: Optional[List[str]] = None) -> List[Path]:
        """çˆ¬å–ç½‘é¡µ"""
        visited = set()
        to_visit = [(url, 0)]
        downloaded_files = []
        
        while to_visit:
            current_url, depth = to_visit.pop(0)
            
            if current_url in visited or depth > max_depth:
                continue
            
            visited.add(current_url)
            
            try:
                # æ£€æŸ¥åŸŸå
                if allowed_domains:
                    domain = urlparse(current_url).netloc
                    if domain not in allowed_domains:
                        continue
                
                # å‘é€è¯·æ±‚
                response = self.session.get(current_url, timeout=10)
                response.raise_for_status()
                
                # è§£æHTML
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # æå–æ–‡æœ¬å†…å®¹
                text_content = soup.get_text(separator='\n', strip=True)
                
                # ä¿å­˜æ–‡ä»¶
                file_path = self._save_content(current_url, text_content)
                downloaded_files.append(file_path)
                
                # å‘ç°æ–°é“¾æ¥
                if depth < max_depth:
                    links = self._extract_links(soup, current_url, allowed_patterns)
                    for link in links:
                        if link not in visited:
                            to_visit.append((link, depth + 1))
                
                # å»¶è¿Ÿ
                time.sleep(random.uniform(*self.delay_range))
                
      <CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_crawl_with_playwright.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
from playwright.sync_api import sync_playwright
from pathlib import Path

def crawl_with_playwright(url: str, output_dir: Path):
 <CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_user.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# éšæœºUser-Agent
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
]

headers = {
    'User-Agent': random.choice(user_agents)
}
```
-->content()
        text = page.inner_text('body')
        
        # ä¿å­˜
        output_file = output_dir / "page.html"
        output_file.write_text(content, encoding='utf-8')
        
        browser.close()
```
-->ut_dir: Path):
    """ä½¿ç”¨Playwrightçˆ¬å–åŠ¨æ€ç½‘é¡µ"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_request_delay.py"
  langua<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_proxy_pool.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# ä½¿ç”¨ä»£ç†
proxi<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_check_robots.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin

def check_robots(url: str) -> bool:
    """æ£€æŸ¥robots.txt"""
    rp = RobotFileParser()
    rp.set_ur<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_web_crawler.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
from extension.python.tools.data_collector import WebCrawler
from pathlib import Path

crawler = WebCrawler(output_dir=Path("data/collected"))

# çˆ¬å–ç½‘é¡µ
files<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_pdf_downloader.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
from extension.python.tools.data_collector import PDFDownloader
from pathlib import Path

downloader = PDFDownloader(<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_academic_scraper.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
from extension.python.tools.data_collector import AcademicScraper
from pathlib import Path

scraper = AcademicScraper(o<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13_source_recommender.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
from extension.python.tools.data_collector import SourceRecommend<CodeFromFile 
  filePath="code_library/010_Chapter10_Development_Guide/10.13/code_10_13__crawl_web.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# é€šè¿‡MCPå·¥å…·è°ƒç”¨
# mcp_servers/data_collector_server.py
async def _crawl_web(self, args: Dict[str, Any]) -> Dict[str, Any]:
    """çˆ¬å–ç½‘é¡µ"""
    url = args["url"]
    output_dir = Path(args["output_dir"])
    max_depth = args.get("max_depth", 2)
    allowed_domains = args.get("allowed_domains")
    
    crawler = WebCrawler(output_dir=output_dir)
    files = crawler.collect(
        url=url,
        max_depth=max_depth,
        allowed_domains=allowed_domains
    )
    
    return {
        "success": True,
        "files_downloaded": len(files),
        "files": [str(f) for f in files]
    }
```
-->botparser import RobotFileParser
from urllib.parse import urljoin

def check_robots(url: str) -> bool:
    """æ£€æŸ¥robots.txt"""
    rp = RobotFileParser()
    rp.set_url(urljoin(url, '/robots.txt'))
    rp.read()
    return rp.can_fetch('*', url)
```

<h2 id="section-10-13-5">ğŸ“¦ 10.13.5 æ•°æ®æ”¶é›†å·¥å…·</h2>

TRQuantç³»ç»Ÿæä¾›äº†å®Œæ•´çš„æ•°æ®æ”¶é›†å·¥å…·åŒ…ã€‚

### WebCrawlerï¼ˆç½‘é¡µçˆ¬è™«ï¼‰

```python
from extension.python.tools.data_collector import WebCrawler
from pathlib import Path

crawler = WebCrawler(output_dir=Path("data/collected"))

# çˆ¬å–ç½‘é¡µ
files = crawler.collect(
    url="https://example.com/docs",
    max_depth=2,
    allowed_domains=["example.com"]
)

print(f"âœ… å…±ä¸‹è½½ {len(files)} ä¸ªæ–‡ä»¶")
```

### PDFDownloaderï¼ˆPDFä¸‹è½½å™¨ï¼‰

```python
from extension.python.tools.data_collector import PDFDownloader
from pathlib import Path

downloader = PDFDownloader(output_dir=Path("data/pdfs"))

# ä¸‹è½½PDF
file_path = downloader.download(
    url="https://example.com/document.pdf",
    filename="document.pdf"
)
```

### AcademicScraperï¼ˆå­¦æœ¯è®ºæ–‡çˆ¬è™«ï¼‰

```python
from extension.python.tools.data_collector import AcademicScraper
from pathlib import Path

scraper = AcademicScraper(output_dir=Path("data/papers"))

# ä»arXivä¸‹è½½è®ºæ–‡
files = scraper.collect(
    database="arxiv",
    query="quantitative+trading",
    max_results=50
)
```

### SourceRecommenderï¼ˆä¿¡æ¯æºæ¨èï¼‰

```python
from extension.python.tools.data_collector import SourceRecommender

recommender = SourceRecommender()

# æ¨èä¿¡æ¯æº
sources = recommender.recommend(
    keywords=["é‡åŒ–æŠ•èµ„", "ç­–ç•¥å¼€å‘"],
    categories=["academic", "blog", "documentation"]
)
```

### MCPå·¥å…·é›†æˆ

```python
# é€šè¿‡MCPå·¥å…·è°ƒç”¨
# mcp_servers/data_collector_server.py
async def _crawl_web(self, args: Dict[str, Any]) -> Dict[str, Any]:
    """çˆ¬å–ç½‘é¡µ"""
    url = args["url"]
    output_dir = Path(args["output_dir"])
    max_depth = args.get("max_depth", 2)
    allowed_domains = args.get("allowed_domains")
    
    crawler = WebCrawler(output_dir=output_dir)
    files = crawler.collect(
        url=url,
        max_depth=max_depth,
        allowed_domains=allowed_domains
    )
    
    return {
        "success": True,
        "files_downloaded": len(files),
        "files": [str(f) for f in files]
    }
```

## ğŸ”— ç›¸å…³ç« èŠ‚

- **10.7 MCPæœåŠ¡å™¨å¼€å‘æŒ‡å—**ï¼šäº†è§£æ•°æ®æ”¶é›†MCPæœåŠ¡å™¨çš„å®ç°
- **10.10 RAGçŸ¥è¯†åº“å¼€å‘æŒ‡å—**ï¼šäº†è§£æ•°æ®æ”¶é›†åœ¨çŸ¥è¯†åº“æ„å»ºä¸­çš„åº”ç”¨
- **10.11 å¼€å‘æµç¨‹æ–¹æ³•è®º**ï¼šäº†è§£æ•°æ®æ”¶é›†åœ¨å¼€å‘æµç¨‹ä¸­çš„ä½¿ç”¨

## ğŸ’¡ å…³é”®è¦ç‚¹

1. **çˆ¬è™«æŠ€æœ¯åŸºç¡€**ï¼šç†è§£çˆ¬è™«å·¥ä½œæµç¨‹å’Œç±»å‹
2. **æŠ€æœ¯æ ˆé€‰æ‹©**ï¼šæ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„å·¥å…·
3. **çˆ¬è™«å®ç°**ï¼šæŒæ¡åŸºç¡€çˆ¬è™«ã€Scrapyã€Playwrightç­‰å®ç°æ–¹æ³•
4. **åçˆ¬ç­–ç•¥åº”å¯¹**ï¼šæŒæ¡å¸¸è§åçˆ¬ç­–ç•¥çš„åº”å¯¹æ–¹æ³•
5. **æ•°æ®æ”¶é›†å·¥å…·**ï¼šæŒæ¡TRQuantæ•°æ®æ”¶é›†å·¥å…·çš„ä½¿ç”¨æ–¹æ³•

## ğŸ”® æ€»ç»“ä¸å±•æœ›

<div class="summary-outlook">
  <h3>æœ¬èŠ‚å›é¡¾</h3>
  <p>æœ¬èŠ‚ç³»ç»Ÿä»‹ç»äº†ç½‘ç»œçˆ¬è™«å¼€å‘ï¼ŒåŒ…æ‹¬çˆ¬è™«æŠ€æœ¯åŸºç¡€ã€æŠ€æœ¯æ ˆé€‰æ‹©ã€çˆ¬è™«å®ç°ã€åçˆ¬ç­–ç•¥åº”å¯¹ã€æ•°æ®æ”¶é›†å·¥å…·ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡ç†è§£ç½‘ç»œçˆ¬è™«å¼€å‘çš„å®Œæ•´æ–¹æ³•ï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡æ•°æ®æ”¶é›†å·¥å…·çš„å¼€å‘æŠ€å·§ã€‚</p>
  
  <h3>ä¸‹èŠ‚é¢„å‘Š</h3>
  <p>å®Œæˆäº†ç¬¬10ç« å¼€å‘æŒ‡å—çš„æ‰€æœ‰å°èŠ‚ã€‚æ¥ä¸‹æ¥å°†è¿›å…¥ç¬¬11ç« ç”¨æˆ·æ‰‹å†Œï¼Œè¯¦ç»†ä»‹ç»ç³»ç»Ÿçš„ä½¿ç”¨æ–¹æ³•ã€æ“ä½œæŒ‡å—ã€åŠŸèƒ½è¯´æ˜ç­‰ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿä¸Šæ‰‹TRQuantç³»ç»Ÿã€‚</p>
  
  <a href="/ashare-book6/011_Chapter11_User_Manual/011_Chapter11_User_Manual_CN" class="next-section">
    ç»§ç»­å­¦ä¹ ï¼šç¬¬11ç«  ç”¨æˆ·æ‰‹å†Œ â†’
  </a>
</div>

> **é€‚ç”¨ç‰ˆæœ¬**: v1.0.0+  
> **æœ€åæ›´æ–°**: 2025-12-12
