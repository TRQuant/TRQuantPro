---
title: "10.10 RAGçŸ¥è¯†åº“å¼€å‘æŒ‡å—"
description: "æ·±å…¥è§£æTRQuant RAGçŸ¥è¯†åº“å¼€å‘ï¼ŒåŒ…æ‹¬RAGæŠ€æœ¯åŸç†ã€çŸ¥è¯†åº“æ¶æ„ã€ç´¢å¼•æ„å»ºã€æ£€ç´¢ç­–ç•¥ã€æ€§èƒ½ä¼˜åŒ–ç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œä¸ºçŸ¥è¯†åº“æ„å»ºå’Œç»´æŠ¤æä¾›å®Œæ•´çš„å¼€å‘æŒ‡å¯¼"
lang: "zh-CN"
layout: "/src/layouts/HandbookLayout.astro"
currentBook: "ashare-book6"
updateDate: "2025-12-12"
---

# ğŸ“š 10.10 RAGçŸ¥è¯†åº“å¼€å‘æŒ‡å—

> **æ ¸å¿ƒæ‘˜è¦ï¼š**
> 
> æœ¬èŠ‚ç³»ç»Ÿä»‹ç»TRQuant RAGçŸ¥è¯†åº“å¼€å‘ï¼ŒåŒ…æ‹¬RAGæŠ€æœ¯åŸç†ã€çŸ¥è¯†åº“æ¶æ„ã€ç´¢å¼•æ„å»ºã€æ£€ç´¢ç­–ç•¥ã€æ€§èƒ½ä¼˜åŒ–ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡ç†è§£RAGçŸ¥è¯†åº“å¼€å‘çš„å®Œæ•´æ–¹æ³•ï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡çŸ¥è¯†åº“çš„æ„å»ºå’Œç»´æŠ¤æŠ€å·§ï¼Œä¸ºæ„å»ºä¸“ä¸šçº§çš„æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚

RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„æŠ€æœ¯ã€‚TRQuantç³»ç»Ÿé€šè¿‡RAGçŸ¥è¯†åº“å®ç°æ™ºèƒ½åŒ–çš„æ–‡æ¡£æ£€ç´¢å’Œä»£ç æ£€ç´¢ï¼Œä¸ºå¼€å‘è¿‡ç¨‹æä¾›ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

## ğŸ“‹ ç« èŠ‚æ¦‚è§ˆ

<script>
function scrollToSection(sectionId) {
  const element = document.getElementById(sectionId);
  if (element) {
    const headerOffset = 100;
    const elementPosition = element.getBoundingClientRect().top;
    const offsetPosition = elementPosition + window.pageYOffset - headerOffset;
    window.scrollTo({
      top: offsetPosition,
      behavior: 'smooth'
    });
  }
}
</script>

<div class="section-overview">
  <div class="section-item" onclick="scrollToSection('section-10-10-1')">
    <h4>ğŸ”¬ 10.10.1 RAGæŠ€æœ¯åŸç†</h4>
    <p>RAGæ¦‚è¿°ã€å·¥ä½œæµç¨‹ã€æ ¸å¿ƒç»„ä»¶ã€æŠ€æœ¯ä¼˜åŠ¿</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-10-2')">
    <h4>ğŸ—ï¸ 10.10.2 çŸ¥è¯†åº“æ¶æ„</h4>
    <p>çŸ¥è¯†åº“ä½“ç³»ã€Manual KBã€Engineering KBã€æ•°æ®æ¥æº</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-10-3')">
    <h4>ğŸ”¨ 10.10.3 ç´¢å¼•æ„å»º</h4>
    <p>æ„å»ºæµç¨‹ã€æ–‡æ¡£åˆ‡åˆ†ã€å‘é‡åŒ–ã€BM25ç´¢å¼•ã€å…ƒæ•°æ®æå–</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-10-4')">
    <h4>ğŸ” 10.10.4 æ£€ç´¢ç­–ç•¥</h4>
    <p>æ··åˆæ£€ç´¢ã€ç»“æœèåˆã€é‡æ’åºã€æŸ¥è¯¢æ‰©å±•ã€ç»“æœè¿‡æ»¤</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-10-10-5')">
    <h4>âš¡ 10.10.5 æ€§èƒ½ä¼˜åŒ–</h4>
    <p>ç´¢å¼•ä¼˜åŒ–ã€æ£€ç´¢ä¼˜åŒ–ã€å­˜å‚¨ä¼˜åŒ–ã€ç¼“å­˜ç­–ç•¥</p>
  </div>
</div>

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- **ç†è§£RAGæŠ€æœ¯**ï¼šæŒæ¡RAGæŠ€æœ¯åŸç†å’Œæ ¸å¿ƒç»„ä»¶
- **è®¾è®¡çŸ¥è¯†åº“æ¶æ„**ï¼šç†è§£çŸ¥è¯†åº“ä½“ç³»ç»“æ„å’Œæ•°æ®ç»„ç»‡
- **æ„å»ºç´¢å¼•**ï¼šæŒæ¡ç´¢å¼•æ„å»ºæµç¨‹å’Œæ–‡æ¡£å¤„ç†æŠ€å·§
- **å®ç°æ£€ç´¢ç­–ç•¥**ï¼šæŒæ¡æ··åˆæ£€ç´¢å’Œç»“æœèåˆæ–¹æ³•
- **ä¼˜åŒ–æ€§èƒ½**ï¼šæŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€å·§å’Œæœ€ä½³å®è·µ

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### RAGæŠ€æœ¯

- **æ£€ç´¢å¢å¼ºç”Ÿæˆ**ï¼šç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆ
- **å‘é‡æ£€ç´¢**ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ£€ç´¢
- **å…³é”®è¯æ£€ç´¢**ï¼šåŸºäºBM25çš„å…³é”®è¯åŒ¹é…
- **é‡æ’åº**ï¼šä½¿ç”¨CrossEncoderæå‡ç›¸å…³æ€§

### çŸ¥è¯†åº“ä½“ç³»

- **Manual KB**ï¼šæ‰‹å†ŒçŸ¥è¯†åº“ï¼ˆå¼€å‘æ‰‹å†Œã€è®¾è®¡æ–‡æ¡£ï¼‰
- **Engineering KB**ï¼šå·¥ç¨‹çŸ¥è¯†åº“ï¼ˆä»£ç ã€APIã€é…ç½®ï¼‰
- **Strategy KB**ï¼šç­–ç•¥çŸ¥è¯†åº“ï¼ˆç ”ç©¶å¡ã€ç­–ç•¥è§„åˆ™ï¼‰

### æ£€ç´¢ç­–ç•¥

- **æ··åˆæ£€ç´¢**ï¼šå‘é‡æ£€ç´¢ + BM25æ£€ç´¢
- **ç»“æœèåˆ**ï¼šReciprocal Rank Fusion (RRF)
- **é‡æ’åº**ï¼šCrossEncoderé‡æ–°æ’åº

<h2 id="section-10-10-1">ğŸ”¬ 10.10.1 RAGæŠ€æœ¯åŸç†</h2>

RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§å¢å¼ºç”Ÿæˆå¼AIçš„æŠ€æœ¯ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³æ–‡æ¡£æ¥å¢å¼ºLLMçš„ç”Ÿæˆèƒ½åŠ›ã€‚

### RAGæ¦‚è¿°

```
ä¼ ç»Ÿç”Ÿæˆå¼AI:
ç”¨æˆ·é—®é¢˜ â†’ LLM â†’ å›ç­”ï¼ˆåŸºäºè®­ç»ƒæ•°æ®ï¼‰

RAGå¢å¼º:
ç”¨æˆ·é—®é¢˜ â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ LLMï¼ˆåŸºäºæ£€ç´¢å†…å®¹ï¼‰ â†’ å›ç­”ï¼ˆåŸºäºæœ€æ–°çŸ¥è¯†ï¼‰
```

### RAGå·¥ä½œæµç¨‹

```
ç”¨æˆ·æŸ¥è¯¢
    â†“
æŸ¥è¯¢ç†è§£
    â†“
å‘é‡æ£€ç´¢ â†â”€â”€â”
    â†“        â”‚
BM25æ£€ç´¢    â”‚
    â†“        â”‚
ç»“æœèåˆ    â”‚
    â†“        â”‚
é‡æ’åº      â”‚
    â†“        â”‚
ä¸Šä¸‹æ–‡æ„å»º  â”‚
    â†“        â”‚
LLMç”Ÿæˆ    â”‚
    â†“        â”‚
æœ€ç»ˆå›ç­”    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    çŸ¥è¯†åº“
```

### æ ¸å¿ƒç»„ä»¶

#### å‘é‡æ•°æ®åº“ï¼ˆVector Databaseï¼‰

```python
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# åˆå§‹åŒ–embeddingæ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={'device': 'cpu'}
)

# åˆ›å»ºå‘é‡åº“
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory="data/kb/manual_kb"
)
```

#### å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰

```python
from rank_bm25 import BM25Okapi

# æ„å»ºBM25ç´¢å¼•
tokenized_docs = [doc.page_content.split() for doc in documents]
bm25 = BM25Okapi(tokenized_docs)

# æ£€ç´¢
query_tokens = query.split()
scores = bm25.get_scores(query_tokens)
top_indices = np.argsort(scores)[-top_k:][::-1]
```

#### é‡æ’åºï¼ˆRerankerï¼‰

```python
from sentence_transformers import CrossEncoder

# åˆå§‹åŒ–reranker
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

# é‡æ’åº
pairs = [[query, doc.page_content[:512]] for doc in results]
scores = reranker.predict(pairs)

# æŒ‰åˆ†æ•°æ’åº
sorted_indices = np.argsort(scores)[::-1]
reranked_results = [results[i] for i in sorted_indices]
```

### RAGçš„ä¼˜åŠ¿

1. **å®æ—¶æ€§**ï¼šå¯ä»¥è®¿é—®æœ€æ–°çŸ¥è¯†ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹
2. **å‡†ç¡®æ€§**ï¼šåŸºäºå®é™…æ–‡æ¡£ï¼Œå‡å°‘å¹»è§‰
3. **å¯è¿½æº¯æ€§**ï¼šå¯ä»¥å¼•ç”¨æ¥æºï¼Œä¾¿äºéªŒè¯
4. **å¯æ‰©å±•æ€§**ï¼šå¯ä»¥è½»æ¾æ·»åŠ æ–°çŸ¥è¯†

<h2 id="section-10-10-2">ğŸ—ï¸ 10.10.2 çŸ¥è¯†åº“æ¶æ„</h2>

TRQuantç³»ç»ŸåŒ…å«ä¸‰ä¸ªçŸ¥è¯†åº“ï¼šManual KBã€Engineering KBå’ŒStrategy KBã€‚

### çŸ¥è¯†åº“ä½“ç³»

```
çŸ¥è¯†åº“ä½“ç³»
â”œâ”€â”€ Manual KB (æ‰‹å†ŒçŸ¥è¯†åº“)
â”‚   â”œâ”€â”€ å¼€å‘æ‰‹å†Œ (ashare-book6/**/*.md)
â”‚   â”œâ”€â”€ è®¾è®¡æ–‡æ¡£ (docs/**/*.md)
â”‚   â””â”€â”€ ä½¿ç”¨æŒ‡å— (extension/AShare-manual/docs/**/*.md)
â”‚
â”œâ”€â”€ Engineering KB (å·¥ç¨‹çŸ¥è¯†åº“)
â”‚   â”œâ”€â”€ ä»£ç æ–‡ä»¶ (core/**, extension/**, mcp_servers/**)
â”‚   â”œâ”€â”€ ç±»å®šä¹‰ (classes)
â”‚   â”œâ”€â”€ å‡½æ•°å®šä¹‰ (functions)
â”‚   â””â”€â”€ é…ç½®ä¿¡æ¯ (configs)
â”‚
â””â”€â”€ Strategy KB (ç­–ç•¥çŸ¥è¯†åº“)
    â”œâ”€â”€ ç ”ç©¶å¡ (research cards)
    â”œâ”€â”€ ç­–ç•¥è§„åˆ™ (strategy rules)
    â””â”€â”€ å›æµ‹ç»“æœ (backtest results)
```

### Manual KBæ¶æ„

#### æ•°æ®æ¥æº

```python
def collect_manual_kb_files() -> List[Path]:
    """æ”¶é›†Manual KBæ–‡ä»¶"""
    files = []
    project_root = Path(__file__).parent.parent.parent
    
    # 1. å¼€å‘æ‰‹å†Œ
    manual_dir = project_root / "extension/AShare-manual/src/pages/ashare-book6"
    files.extend(manual_dir.rglob("*.md"))
    
    # 2. è®¾è®¡æ–‡æ¡£
    docs_dir = project_root / "extension/AShare-manual/docs"
    files.extend(docs_dir.rglob("*.md"))
    
    # 3. å…¶ä»–æ–‡æ¡£
    other_docs = project_root / "docs"
    if other_docs.exists():
        files.extend(other_docs.rglob("*.md"))
    
    return files
```

#### æ–‡æ¡£åˆ‡åˆ†

```python
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

def chunk_markdown(file_path: Path) -> List[Document]:
    """åˆ‡åˆ†Markdownæ–‡æ¡£"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŒ‰æ ‡é¢˜åˆ‡åˆ†
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on
    )
    
    chunks = markdown_splitter.split_text(content)
    
    # è¿›ä¸€æ­¥åˆ‡åˆ†ï¼ˆå¦‚æœchunkå¤ªå¤§ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    
    all_chunks = []
    for chunk in chunks:
        sub_chunks = text_splitter.split_documents([chunk])
        all_chunks.extend(sub_chunks)
    
    return all_chunks
```

### Engineering KBæ¶æ„

#### ä»£ç è§£æ

```python
import ast
from langchain.schema import Document

def extract_symbols(file_path: Path) -> List[Document]:
    """æå–ä»£ç ç¬¦å·ï¼ˆç±»ã€å‡½æ•°ï¼‰"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    try:
        tree = ast.parse(content)
        symbols = []
        
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                # æå–ç±»å®šä¹‰
                docstring = ast.get_docstring(node) or ""
                code = ast.get_source_segment(content, node)
                
                symbols.append(Document(
                    page_content=f"{node.name}\n\n{docstring}\n\n{code}",
                    metadata={
                        "type": "class",
                        "name": node.name,
                        "file_path": str(file_path),
                        "line": node.lineno
                    }
                ))
            
            elif isinstance(node, ast.FunctionDef):
                # æå–å‡½æ•°å®šä¹‰
                docstring = ast.get_docstring(node) or ""
                code = ast.get_source_segment(content, node)
                
                symbols.append(Document(
                    page_content=f"{node.name}\n\n{docstring}\n\n{code}",
                    metadata={
                        "type": "function",
                        "name": node.name,
                        "file_path": str(file_path),
                        "line": node.lineno
                    }
                ))
        
        return symbols
    
    except SyntaxError:
        return []
```

<h2 id="section-10-10-3">ğŸ”¨ 10.10.3 ç´¢å¼•æ„å»º</h2>

ç´¢å¼•æ„å»ºåŒ…æ‹¬æ–‡æ¡£æ”¶é›†ã€åˆ‡åˆ†ã€å‘é‡åŒ–ã€BM25ç´¢å¼•æ„å»ºç­‰æ­¥éª¤ã€‚

### æ„å»ºæµç¨‹

```
æ”¶é›†æ–‡ä»¶
    â†“
æ–‡æ¡£åˆ‡åˆ†
    â†“
æå–å…ƒæ•°æ®
    â†“
ç”Ÿæˆå‘é‡
    â†“
æ„å»ºç´¢å¼•
  â”œâ”€â”€ Chroma (å‘é‡ç´¢å¼•)
  â””â”€â”€ BM25 (å…³é”®è¯ç´¢å¼•)
    â†“
ä¿å­˜ç´¢å¼•
```

### Manual KBç´¢å¼•æ„å»º

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""æ„å»ºManual KBç´¢å¼•"""
from pathlib import Path
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from rank_bm25 import BM25Okapi
import json
import pickle

def build_manual_kb_index():
    """æ„å»ºManual KBç´¢å¼•"""
    
    # 1. æ”¶é›†æ–‡ä»¶
    files = collect_manual_kb_files()
    print(f"âœ… å…±æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    
    # 2. åˆ‡åˆ†æ–‡æ¡£
    all_documents = []
    for file_path in files:
        chunks = chunk_markdown(file_path)
        # æ·»åŠ å…ƒæ•°æ®
        for chunk in chunks:
            chunk.metadata.update(extract_metadata(file_path))
        all_documents.extend(chunks)
    
    print(f"âœ… å…±ç”Ÿæˆ {len(all_documents)} ä¸ªchunks")
    
    # 3. æ„å»ºå‘é‡ç´¢å¼•
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    vectorstore = Chroma.from_documents(
        documents=all_documents,
        embedding=embeddings,
        persist_directory="data/kb/manual_kb"
    )
    
    # 4. æ„å»ºBM25ç´¢å¼•
    tokenized_docs = [doc.page_content.split() for doc in all_documents]
    bm25_index = BM25Okapi(tokenized_docs)
    
    # 5. ä¿å­˜ç´¢å¼•
    with open("data/kb/manual_kb/bm25_index.pkl", 'wb') as f:
        pickle.dump(bm25_index, f)
    
    with open("data/kb/manual_kb/documents.json", 'w', encoding='utf-8') as f:
        json.dump([doc.dict() for doc in all_documents], f, ensure_ascii=False, indent=2)
    
    print("âœ… Manual KBç´¢å¼•æ„å»ºå®Œæˆ")

if __name__ == "__main__":
    build_manual_kb_index()
```

### å…ƒæ•°æ®æå–

```python
import re
from datetime import datetime

def extract_metadata(file_path: Path) -> Dict[str, Any]:
    """æå–æ–‡æ¡£å…ƒæ•°æ®"""
    rel_path = file_path.relative_to(project_root)
    
    # ä»è·¯å¾„æå–ä¿¡æ¯
    parts = rel_path.parts
    metadata = {
        "file_path": str(rel_path),
        "doc_id": file_path.stem,
        "lang": "zh" if "_CN" in file_path.name else "en",
        "updated_at": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
    }
    
    # æå–ç« èŠ‚ä¿¡æ¯
    if "Chapter" in str(rel_path):
        chapter_match = re.search(r'(\d+)_Chapter', str(rel_path))
        if chapter_match:
            metadata["chapter"] = int(chapter_match.group(1))
    
    return metadata
```

<h2 id="section-10-10-4">ğŸ” 10.10.4 æ£€ç´¢ç­–ç•¥</h2>

æ£€ç´¢ç­–ç•¥åŒ…æ‹¬æ··åˆæ£€ç´¢ã€ç»“æœèåˆã€é‡æ’åºç­‰ã€‚

### æ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰

```python
class KBServer:
    """çŸ¥è¯†åº“æœåŠ¡å™¨"""
    
    def __init__(self):
        self.manual_vectorstore = None
        self.engineering_vectorstore = None
        self.manual_bm25 = None
        self.engineering_bm25 = None
        self.manual_docs = None
        self.engineering_docs = None
        self.reranker = None
        self._load_indices()
    
    def query(
        self,
        query: str,
        scope: str = "both",  # "manual", "engineering", "both"
        top_k: int = 10,
        use_reranker: bool = False
    ) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        
        # 1. å‘é‡æ£€ç´¢
        vector_results = self._vector_search(query, scope, top_k * 2)
        
        # 2. BM25æ£€ç´¢
        bm25_results = self._bm25_search(query, scope, top_k * 2)
        
        # 3. ç»“æœèåˆ
        merged_results = self._merge_results(vector_results, bm25_results)
        
        # 4. é‡æ’åºï¼ˆå¯é€‰ï¼‰
        if use_reranker:
            merged_results = self._rerank_results(query, merged_results, top_k)
        else:
            merged_results = merged_results[:top_k]
        
        return merged_results
    
    def _vector_search(self, query: str, scope: str, top_k: int) -> List[Dict]:
        """å‘é‡æ£€ç´¢"""
        results = []
        
        if scope in ["manual", "both"] and self.manual_vectorstore:
            vector_results = self.manual_vectorstore.similarity_search_with_score(
                query, k=top_k
            )
            for doc, score in vector_results:
                results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score),
                    "source": "manual",
                    "method": "vector"
                })
        
        if scope in ["engineering", "both"] and self.engineering_vectorstore:
            vector_results = self.engineering_vectorstore.similarity_search_with_score(
                query, k=top_k
            )
            for doc, score in vector_results:
                results.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score),
                    "source": "engineering",
                    "method": "vector"
                })
        
        return results
    
    def _bm25_search(self, query: str, scope: str, top_k: int) -> List[Dict]:
        """BM25æ£€ç´¢"""
        results = []
        query_tokens = query.lower().split()
        
        if scope in ["manual", "both"] and self.manual_bm25 and self.manual_docs:
            bm25_scores = self.manual_bm25.get_scores(query_tokens)
            top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
            
            for idx in top_indices:
                if bm25_scores[idx] > 0:
                    doc = self.manual_docs[idx]
                    results.append({
                        "content": doc.get("page_content", ""),
                        "metadata": doc.get("metadata", {}),
                        "score": float(bm25_scores[idx]),
                        "source": "manual",
                        "method": "bm25"
                    })
        
        if scope in ["engineering", "both"] and self.engineering_bm25 and self.engineering_docs:
            bm25_scores = self.engineering_bm25.get_scores(query_tokens)
            top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
            
            for idx in top_indices:
                if bm25_scores[idx] > 0:
                    doc = self.engineering_docs[idx]
                    results.append({
                        "content": doc.get("page_content", ""),
                        "metadata": doc.get("metadata", {}),
                        "score": float(bm25_scores[idx]),
                        "source": "engineering",
                        "method": "bm25"
                    })
        
        return results
    
    def _merge_results(self, vector_results: List[Dict], bm25_results: List[Dict]) -> List[Dict]:
        """èåˆç»“æœï¼ˆä½¿ç”¨Reciprocal Rank Fusionï¼‰"""
        combined = {}
        
        # å‘é‡æ£€ç´¢ç»“æœ
        for i, result in enumerate(vector_results):
            doc_id = result["metadata"].get("doc_id", f"{result['source']}_{i}")
            if doc_id not in combined:
                combined[doc_id] = {
                    "result": result,
                    "vector_rank": i + 1,
                    "bm25_rank": None
                }
            else:
                combined[doc_id]["vector_rank"] = i + 1
        
        # BM25æ£€ç´¢ç»“æœ
        for i, result in enumerate(bm25_results):
            doc_id = result["metadata"].get("doc_id", f"{result['source']}_{i}")
            if doc_id not in combined:
                combined[doc_id] = {
                    "result": result,
                    "vector_rank": None,
                    "bm25_rank": i + 1
                }
            else:
                combined[doc_id]["bm25_rank"] = i + 1
        
        # è®¡ç®—RRFåˆ†æ•°
        for doc_id, info in combined.items():
            rrf_score = 0
            if info["vector_rank"]:
                rrf_score += 1.0 / (60 + info["vector_rank"])
            if info["bm25_rank"]:
                rrf_score += 1.0 / (60 + info["bm25_rank"])
            info["result"]["rrf_score"] = rrf_score
        
        # æŒ‰RRFåˆ†æ•°æ’åº
        sorted_results = sorted(
            combined.values(),
            key=lambda x: x["result"]["rrf_score"],
            reverse=True
        )
        return [item["result"] for item in sorted_results]
    
    def _rerank_results(
        self,
        query: str,
        results: List[Dict[str, Any]],
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨rerankeré‡æ–°æ’åºç»“æœ"""
        if not self.reranker:
            try:
                from sentence_transformers import CrossEncoder
                self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            except ImportError:
                return results[:top_k]
        
        pairs = [[query, result["content"][:512]] for result in results]
        scores = self.reranker.predict(pairs)
        
        # æ›´æ–°åˆ†æ•°å¹¶æ’åº
        for i, result in enumerate(results):
            result["rerank_score"] = float(scores[i])
            result["score"] = float(scores[i])
        
        results.sort(key=lambda x: x["rerank_score"], reverse=True)
        return results[:top_k]
```

<h2 id="section-10-10-5">âš¡ 10.10.5 æ€§èƒ½ä¼˜åŒ–</h2>

æ€§èƒ½ä¼˜åŒ–åŒ…æ‹¬ç´¢å¼•ä¼˜åŒ–ã€æ£€ç´¢ä¼˜åŒ–ã€å­˜å‚¨ä¼˜åŒ–ç­‰ã€‚

### ç´¢å¼•ä¼˜åŒ–

```python
# ä½¿ç”¨æ›´å°çš„chunk size
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,  # å‡å°chunk size
    chunk_overlap=100
)

# ä½¿ç”¨æ›´å¿«çš„embeddingæ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={'device': 'cpu'}  # æˆ– 'cuda' å¦‚æœæœ‰GPU
)
```

### æ£€ç´¢ä¼˜åŒ–

```python
# ç¼“å­˜æŸ¥è¯¢ç»“æœ
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_query(query: str, scope: str, top_k: int):
    """ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
    return kb.query(query, scope, top_k)

# å¼‚æ­¥æ£€ç´¢
import asyncio

async def async_query(query: str, scope: str, top_k: int):
    """å¼‚æ­¥æŸ¥è¯¢"""
    tasks = []
    if scope in ["manual", "both"]:
        tasks.append(self._async_vector_search(query, "manual", top_k))
    if scope in ["engineering", "both"]:
        tasks.append(self._async_vector_search(query, "engineering", top_k))
    
    results = await asyncio.gather(*tasks)
    return self._merge_results(*results)
```

### å­˜å‚¨ä¼˜åŒ–

```python
# å‹ç¼©å‘é‡å­˜å‚¨
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory="data/kb/manual_kb",
    collection_metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
)
```

## ğŸ”— ç›¸å…³ç« èŠ‚

- **10.7 MCPæœåŠ¡å™¨å¼€å‘æŒ‡å—**ï¼šäº†è§£çŸ¥è¯†åº“MCPæœåŠ¡å™¨çš„å®ç°
- **10.9 MCP Ã— Cursor Ã— å·¥å…·é“¾è”ç”¨è§„èŒƒ**ï¼šäº†è§£çŸ¥è¯†åº“åœ¨å·¥å…·é“¾ä¸­çš„ä½¿ç”¨
- **ç¬¬1ç« ï¼šç³»ç»Ÿæ¦‚è¿°**ï¼šäº†è§£ç³»ç»Ÿæ•´ä½“æ¶æ„

## ğŸ’¡ å…³é”®è¦ç‚¹

1. **RAGæŠ€æœ¯åŸç†**ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®è¯æ£€ç´¢
2. **çŸ¥è¯†åº“æ¶æ„**ï¼šManual KBã€Engineering KBã€Strategy KBä¸‰ä¸ªçŸ¥è¯†åº“
3. **ç´¢å¼•æ„å»º**ï¼šæ–‡æ¡£åˆ‡åˆ†ã€å‘é‡åŒ–ã€BM25ç´¢å¼•æ„å»º
4. **æ£€ç´¢ç­–ç•¥**ï¼šæ··åˆæ£€ç´¢ã€ç»“æœèåˆã€é‡æ’åº
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šç´¢å¼•ä¼˜åŒ–ã€æ£€ç´¢ä¼˜åŒ–ã€å­˜å‚¨ä¼˜åŒ–

## ğŸ”® æ€»ç»“ä¸å±•æœ›

<div class="summary-outlook">
  <h3>æœ¬èŠ‚å›é¡¾</h3>
  <p>æœ¬èŠ‚ç³»ç»Ÿä»‹ç»äº†RAGçŸ¥è¯†åº“å¼€å‘ï¼ŒåŒ…æ‹¬RAGæŠ€æœ¯åŸç†ã€çŸ¥è¯†åº“æ¶æ„ã€ç´¢å¼•æ„å»ºã€æ£€ç´¢ç­–ç•¥ã€æ€§èƒ½ä¼˜åŒ–ç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡ç†è§£RAGçŸ¥è¯†åº“å¼€å‘çš„å®Œæ•´æ–¹æ³•ï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡çŸ¥è¯†åº“çš„æ„å»ºå’Œç»´æŠ¤æŠ€å·§ã€‚</p>
  
  <h3>ä¸‹èŠ‚é¢„å‘Š</h3>
  <p>æŒæ¡äº†RAGçŸ¥è¯†åº“å¼€å‘åï¼Œä¸‹ä¸€èŠ‚å°†ä»‹ç»å¼€å‘æµç¨‹æ–¹æ³•è®ºï¼ŒåŒ…æ‹¬é—®é¢˜è¯†åˆ«ã€æ·±å…¥ç ”ç©¶ã€æ–¹æ¡ˆè®¾è®¡ã€å®ç°éªŒè¯ã€æ–‡æ¡£åŒ–ç­‰ã€‚é€šè¿‡ç†è§£å¼€å‘æµç¨‹æ–¹æ³•è®ºï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡ç³»ç»ŸåŒ–çš„å¼€å‘æ–¹æ³•ã€‚</p>
  
  <a href="/ashare-book6/010_Chapter10_Development_Guide/10.11_Development_Methodology_CN" class="next-section">
    ç»§ç»­å­¦ä¹ ï¼š10.11 å¼€å‘æµç¨‹æ–¹æ³•è®º â†’
  </a>
</div>

> **é€‚ç”¨ç‰ˆæœ¬**: v1.0.0+  
> **æœ€åæ›´æ–°**: 2025-12-12
