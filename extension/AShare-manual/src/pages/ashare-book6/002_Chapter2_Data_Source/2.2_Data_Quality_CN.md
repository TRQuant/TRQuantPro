---
title: "2.2 æ•°æ®è´¨é‡"
description: "æ·±å…¥è§£ææ•°æ®è´¨é‡æ£€æŸ¥æœºåˆ¶ï¼ŒåŒ…æ‹¬å®Œæ•´æ€§æ£€æŸ¥ã€å‡†ç¡®æ€§éªŒè¯ã€å¼‚å¸¸æ£€æµ‹ã€æ•°æ®æ¸…æ´—å’Œè´¨é‡æŠ¥å‘Šç”Ÿæˆ"
lang: "zh-CN"
layout: "/src/layouts/HandbookLayout.astro"
currentBook: "ashare-book6"
updateDate: "2025-12-12"
---

# âœ… 2.2 æ•°æ®è´¨é‡

> **æ ¸å¿ƒæ‘˜è¦ï¼š**
> 
> æœ¬èŠ‚ç³»ç»Ÿä»‹ç»TRQuantç³»ç»Ÿçš„æ•°æ®è´¨é‡æ£€æŸ¥æœºåˆ¶ï¼ŒåŒ…æ‹¬å®Œæ•´æ€§æ£€æŸ¥ã€å‡†ç¡®æ€§éªŒè¯ã€å¼‚å¸¸æ£€æµ‹å’Œæ•°æ®æ¸…æ´—ã€‚é€šè¿‡ç†è§£æ•°æ®è´¨é‡æ£€æŸ¥çš„ä¸‰ä¸ªç»´åº¦ã€å„ç§æ£€æµ‹æ–¹æ³•ã€è‡ªåŠ¨æ¸…æ´—ç­–ç•¥å’Œè´¨é‡æŠ¥å‘Šç”Ÿæˆï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡æ•°æ®è´¨é‡ä¿è¯çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œç¡®ä¿åç»­åˆ†æä½¿ç”¨çš„æ•°æ®è´¨é‡å¯é ã€‚

## ğŸ“‹ ç« èŠ‚æ¦‚è§ˆ

<script>
function scrollToSection(sectionId) {
  const element = document.getElementById(sectionId);
  if (element) {
    const headerOffset = 100;
    const elementPosition = element.getBoundingClientRect().top;
    const offsetPosition = elementPosition + window.pageYOffset - headerOffset;
    window.scrollTo({
      top: offsetPosition,
      behavior: 'smooth'
    });
  }
}
</script>

<div class="section-overview">
  <div class="section-item" onclick="scrollToSection('section-2-2-1')">
    <h4>ğŸ“Š 2.2.1 å®Œæ•´æ€§æ£€æŸ¥</h4>
    <p>ç¼ºå¤±å€¼æ£€æŸ¥ã€æ—¶é—´åºåˆ—è¿ç»­æ€§ã€å­—æ®µå®Œæ•´æ€§éªŒè¯</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-2-2-2')">
    <h4>âœ… 2.2.2 å‡†ç¡®æ€§éªŒè¯</h4>
    <p>æ•°æ®èŒƒå›´æ£€æŸ¥ã€ä¸šåŠ¡é€»è¾‘éªŒè¯ã€å¤šæºæ•°æ®ä¸€è‡´æ€§</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-2-2-3')">
    <h4>ğŸ” 2.2.3 å¼‚å¸¸æ£€æµ‹</h4>
    <p>ç»Ÿè®¡æ–¹æ³•ã€æœºå™¨å­¦ä¹ æ–¹æ³•ã€å¼‚å¸¸æ¨¡å¼è¯†åˆ«</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-2-2-4')">
    <h4>ğŸ§¹ 2.2.4 æ•°æ®æ¸…æ´—</h4>
    <p>ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼å¤„ç†ã€æ•°æ®æ ‡å‡†åŒ–</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-2-2-5')">
    <h4>ğŸ“ˆ 2.2.5 è´¨é‡æŠ¥å‘Š</h4>
    <p>è´¨é‡è¯„åˆ†ã€è´¨é‡æŠ¥å‘Šç”Ÿæˆã€è´¨é‡ç›‘æ§</p>
  </div>
  <div class="section-item" onclick="scrollToSection('section-2-2-6')">
    <h4>ğŸ› ï¸ 2.2.6 MCPå·¥å…·ä½¿ç”¨</h4>
    <p>ä½¿ç”¨MCPå·¥å…·ç ”ç©¶æ•°æ®è´¨é‡æ–¹æ³•ã€æ”¶é›†è´¨é‡æ£€æŸ¥èµ„æ–™</p>
  </div>
</div>

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- **æŒæ¡å®Œæ•´æ€§æ£€æŸ¥**ï¼šç†è§£ç¼ºå¤±å€¼æ£€æŸ¥ã€æ—¶é—´åºåˆ—è¿ç»­æ€§æ£€æŸ¥çš„æ–¹æ³•
- **ç†è§£å‡†ç¡®æ€§éªŒè¯**ï¼šæŒæ¡æ•°æ®èŒƒå›´æ£€æŸ¥ã€ä¸šåŠ¡é€»è¾‘éªŒè¯çš„æŠ€æœ¯
- **ç†Ÿæ‚‰å¼‚å¸¸æ£€æµ‹**ï¼šäº†è§£ç»Ÿè®¡æ–¹æ³•å’Œæœºå™¨å­¦ä¹ æ–¹æ³•åœ¨å¼‚å¸¸æ£€æµ‹ä¸­çš„åº”ç”¨
- **å®ç°æ•°æ®æ¸…æ´—**ï¼šæŒæ¡ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼å¤„ç†ã€æ•°æ®æ ‡å‡†åŒ–çš„æ–¹æ³•
- **ç”Ÿæˆè´¨é‡æŠ¥å‘Š**ï¼šç†è§£è´¨é‡è¯„åˆ†ä½“ç³»å’ŒæŠ¥å‘Šç”Ÿæˆæœºåˆ¶
- **ä½¿ç”¨MCPå·¥å…·**ï¼šæŒæ¡ä½¿ç”¨MCPå·¥å…·è¿›è¡Œæ•°æ®è´¨é‡ç›¸å…³ç ”ç©¶

<h2 id="section-2-2-1">ğŸ“Š 2.2.1 å®Œæ•´æ€§æ£€æŸ¥</h2>

æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ç¡®ä¿æ•°æ®æ²¡æœ‰ç¼ºå¤±ï¼Œæ—¶é—´åºåˆ—è¿ç»­ï¼Œæ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨ã€‚

### æ£€æŸ¥ç»´åº¦

<div class="key-points">
  <div class="key-point">
    <h4>ğŸ” ç¼ºå¤±å€¼æ£€æŸ¥</h4>
    <p>æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦å­˜åœ¨ç¼ºå¤±å€¼ï¼ˆNaNã€Noneã€ç©ºå­—ç¬¦ä¸²ç­‰ï¼‰</p>
  </div>
  <div class="key-point">
    <h4>ğŸ“… æ—¶é—´åºåˆ—è¿ç»­æ€§</h4>
    <p>æ£€æŸ¥æ—¶é—´åºåˆ—æ˜¯å¦è¿ç»­ï¼Œæ˜¯å¦å­˜åœ¨ç¼ºå¤±çš„äº¤æ˜“æ—¥</p>
  </div>
  <div class="key-point">
    <h4>ğŸ“‹ å­—æ®µå®Œæ•´æ€§</h4>
    <p>æ£€æŸ¥æ‰€æœ‰å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œå­—æ®µç±»å‹æ˜¯å¦æ­£ç¡®</p>
  </div>
  <div class="key-point">
    <h4>ğŸ“Š æ•°æ®è¦†ç›–åº¦</h4>
    <p>æ£€æŸ¥æ•°æ®è¦†ç›–çš„æ—¶é—´èŒƒå›´å’Œè‚¡ç¥¨èŒƒå›´æ˜¯å¦å®Œæ•´</p>
  </div>
</div>

### ç¼ºå¤±å€¼æ£€æŸ¥

ç¼ºå¤±å€¼æ£€æŸ¥æ˜¯æ•°æ®å®Œæ•´æ€§æ£€æŸ¥çš„åŸºç¡€ï¼Œè¯†åˆ«æ•°æ®ä¸­çš„ç¼ºå¤±å€¼ï¼š

<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_check_completeness.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_check_completeness.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
import pandas as pd
import numpy as np
from typing import Dict, List, Any

class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
    
    def check_completeness(self, data: pd.DataFrame, 
                          required_fields: List[str] = None) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        
        Args:
            data: å¾…æ£€æŸ¥çš„æ•°æ®
            required_fields: å¿…éœ€å­—æ®µåˆ—è¡¨
        
        Returns:
            å®Œæ•´æ€§æ£€æŸ¥ç»“æœ
        """
        result = {
            "missing_values": 0,
            "missing_fields": [],
            "time_series_continuous": True,
            "completeness_score": 0.0,
            "details": {}
        }
        
        # 1. æ£€æŸ¥ç¼ºå¤±å€¼
        missing_count = data.isnull().sum()
        result["missing_values"] = int(missing_count.sum())
        result["details"]["missing_by_field"] = missing_count.to_dict()
        
        # 2. æ£€æŸ¥å¿…éœ€å­—æ®µ
        if required_fields:
            missing_fields = [f for f in required_fields if f not in data.columns]
            result["missing_fields"] = missing_fields
        
        # 3. æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§
        if 'date' in data.columns or data.index.name == 'date':
            result["time_series_continuous"] = self._check_time_series_continuity(data)
        
        # 4. è®¡ç®—å®Œæ•´æ€§å¾—åˆ†
        total_cells = data.shape[0] * data.shape[1]
        if total_cells > 0:
            result["completeness_score"] = 1.0 - (result["missing_values"] / total_cells)
        
        return result
    
    def _check_time_series_continuity(self, data: pd.DataFrame) -> bool:
        """æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§"""
        # è·å–æ—¥æœŸç´¢å¼•
        if data.index.name == 'date' or 'date' in data.columns:
            if data.index.name == 'date':
                dates = pd.to_datetime(data.index)
            else:
                dates = pd.to_datetime(data['date'])
            
            # ç”Ÿæˆå®Œæ•´çš„äº¤æ˜“æ—¥åºåˆ—
            start_date = dates.min()
            end_date = dates.max()
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µç”Ÿæˆäº¤æ˜“æ—¥åºåˆ—
            # ç®€åŒ–ç¤ºä¾‹ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„æ—¥æœŸ
            date_range = pd.date_range(start=start_date, end=end_date, freq='D')
            missing_dates = set(date_range) <CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_check_time_series_continuity.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```
-->python
def check_time_series_continuity(self, data: pd.DataFrame, 
                                 trading_calendar: List[str] = None) -> Dict[str, Any]:
    """
    æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§
    
    Args:
        data: æ—¶é—´åºåˆ—æ•°æ®
        trading_calendar: äº¤æ˜“æ—¥å†ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        è¿ç»­æ€§æ£€æŸ¥ç»“æœ
    """
    result = {
        "is_continuous": True,
        "missing_dates": [],
        "duplicate_dates": [],
        "continuity_score": 1.0
    }
    
    # è·å–æ—¥æœŸ
    if data.index.name == 'date':
        dates = pd.to_datetime(data.index)
    elif 'date' in data.columns:
        dates = pd.to_datetime(data['date'])
    else:
        return result
    
    # æ£€æŸ¥é‡å¤æ—¥æœŸ
    duplicate_dates = dates[dates.duplicated()].tolist()
    result["duplicate_dates"] = [str(d) for d in duplicate_dates]
    
    # æ£€æŸ¥ç¼ºå¤±æ—¥æœŸ
    if trading_calendar:
        # ä½¿ç”¨äº¤æ˜“æ—¥å†æ£€æŸ¥
        expected_dates = pd.to_datetime(trading_calendar)
        missing_dates = set(expected_dates) - set(dates)
        result["missing_dates"] = [str(d) for d in sorted(missing_dates)]
    else:
        # ç®€å•æ£€æŸ¥ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å¤§çš„æ—¶é—´é—´éš”
        date_diff = dates.diff()
        large_gaps = date_diff[date_diff > pd.Timedelta(days=3)]
        if len(large_gaps) > 0:
            result["is_continuous"] = False
            result["missing_dates"] = [str(dates.iloc[i]) for i in large_gaps.index]
    
    # è®¡ç®—è¿ç»­æ€§å¾—åˆ†
    if len(dates) > 0:
        if trading_calendar:
            expected_count = len(trading_calendar)
            actual_count = len(dates)
            result<CodeFromFile 
  filePath="code<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_check_field_completeness.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def check_field_completeness(self, data: pd.DataFrame, 
                             required_fields: List[str],
                             field_types: Dict[str, type] = None) -> Dict[str, Any]:
    """
    æ£€æŸ¥å­—æ®µå®Œæ•´æ€§
    
    Args:
        data: å¾…æ£€æŸ¥çš„æ•°æ®
        required_fields: å¿…éœ€å­—æ®µåˆ—è¡¨
        field_types: å­—æ®µç±»å‹è¦æ±‚
    
    Returns:
        å­—æ®µå®Œæ•´æ€§æ£€æŸ¥ç»“æœ
    """
    result = {
        "missing_fields": [],
        "type_mismatches": [],
        "completeness_score": 1.0
    }
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    for field in required_fields:
        if field not in data.columns:
            result["missing_fields"].append(field)
    
    # æ£€æŸ¥å­—æ®µç±»å‹
    if field_types:
        for field, expected_type in field_types.items():
            if field in data.columns:
                actual_type = data[field].dtype
                if not self._is_type_compatible(actual_type, expected_type):
                    result["type_mismatches"].append({
                        "field": field,
                        "expected": str(expected_type),
                        "actual": str(actual_type)
                    })
    
    # è®¡ç®—å®Œæ•´æ€§å¾—åˆ†
    total_fields = len(required_fields)
    if total_fields > 0:
        missing_count = len(result["missing_fields"])
        type_mismatch_count = len(result["type_mismatches"])
        result["completeness_score"] = 1.0 - (missing_count + type_mismatch_count) / total_fields
    
    return result
```
--> type_mismatch_count = len(result["type_mismatches"])
        result["completeness_score"] = 1.0 - (missing_count + type_mismatch_count) / total_fields
    
    return result
```
-->cted": str(expected_type),
                        "actual": str(actual_type)
                    })
    
    # è®¡ç®—å®Œæ•´æ€§å¾—åˆ†
    total_f<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_validate_data_range.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def validate_data_range(self, data: pd.DataFrame, 
                       range_rules: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
    """
    éªŒè¯æ•°æ®èŒƒå›´
    
    Args:
        data: å¾…éªŒè¯çš„æ•°æ®
        range_rules: èŒƒå›´è§„åˆ™ï¼Œæ ¼å¼ï¼š{"field": {"min": 0, "max": 100}}
    
    Returns:
        èŒƒå›´éªŒè¯ç»“æœ
    """
    result = {
        "violations": [],
        "violation_count": 0,
        "accuracy_score": 1.0
    }
    
    for field, rules in range_rules.items():
        if field not in data.columns:
            continue
        
        min_val = rules.get("min")
        max_val = rules.get("max")
        
        # æ£€æŸ¥æœ€å°å€¼
        if min_val is not None:
            below_min = data[data[field] < min_val]
            if len(below_min) > 0:
                result["violations"].append({
                    "field": field,
                    "rule": f">= {min_val}",
                    "violation_count": len(below_min),
                    "violation_indices": below_min.index.tolist()
                })
        
        # æ£€æŸ¥æœ€å¤§å€¼
        if max_val is not None:
            above_max = data[data[field] > max_val]
            if len(above_max) > 0:
                result["violations"].append({
                    "field": field,
                    "rule": f"<= {max_val}",
                    "violation_count": len(above_max),
                    "violation_indices": above_max.index.tolist()
                })
    
    result["violation_count"] = len(result["violations"])
    
    # è®¡ç®—å‡†ç¡®æ€§å¾—åˆ†
    total_rows = len(data)
    if total_rows > 0:
        violation_rows = len(set(
            i<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_validate_business_logic.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```
-->ount"] = len(result["violations"])
    
    # è®¡ç®—å‡†ç¡®æ€§å¾—åˆ†
    total_rows = len(data)
    if total_rows > 0:
        violation_rows = len(set(
            i<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_validate_business_logic.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def validate_business_logic(self, data: pd.DataFrame, 
                           rules: Dict[str, str]) -> Dict[str, Any]:
    """
    éªŒè¯ä¸šåŠ¡é€»è¾‘
    
    Args:
        data: å¾…éªŒè¯çš„æ•°æ®
        rules: ä¸šåŠ¡è§„åˆ™ï¼Œæ ¼å¼ï¼š{"rule": "high >= low", "description": "æœ€é«˜ä»·åº”å¤§äºç­‰äºæœ€ä½ä»·"}
    
    Returns:
        ä¸šåŠ¡é€»è¾‘éªŒè¯ç»“æœ
    """
    result = {
        "violations": [],
        "violation_count": 0,
        "accuracy_score": 1.0
    }
    
    for rule_expr, description in rules.items():
        try:
            # ä½¿ç”¨evalè¯„ä¼°è§„åˆ™è¡¨è¾¾å¼
            # æ³¨æ„ï¼šå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
            mask = data.eval(rule_expr)
            violations = data[~mask]
            
            if len(violations) > 0:
                result["violations"].append({
                    "rule": rule_expr,
                    "description": description,
                    "violation_count": len(violations),
                    "violation_<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_check_cross_source_consistency.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def check_cross_source_consistency(self, data_sources: Dict[str, pd.DataFrame],
                                   tolerance: float = 0.01) -> Dict[str, Any]:
    """
    æ£€æŸ¥å¤šæºæ•°æ®ä¸€è‡´æ€§
    
    Args:
        data_sources: å¤šä¸ªæ•°æ®æºçš„æ•°æ®ï¼Œæ ¼å¼ï¼š{"source1": data1, "source2": data2}
        tolerance: å…è®¸çš„å·®å¼‚å®¹å¿åº¦
    
    Returns:
        ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
    """
    result = {
        "inconsistent_fields": [],
        "inconsistency_count": 0,
        "consistency_score": 1.0
    }
    
    if len(data_sources) < 2:
        return result
    
    # è·å–æ‰€æœ‰æ•°æ®æºçš„å…¬å…±å­—æ®µå’Œå…¬å…±ç´¢å¼•
    sources_list = list(data_sources.values())
    common_fields = set(sources_list[0].columns)
    for source_data in sources_list[1:]:
        common_fields &= set(source_data.columns)
    
    common_index = set(sources_list[0].index)
    for source_data in sources_list[1:]:
        common_index &= set(source_data.index)
    
    # æ¯”è¾ƒæ¯ä¸ªå­—æ®µ
    for field in common_fields:
        field_data = {}
        for source_name, source_data in data_sources.items():
            field_data[source_name] = source_data.loc[list(common_index), field]
        
        # è®¡ç®—å·®å¼‚
        source_names = list(field_data.keys())
        for i in range(len(source_names)):
            for j in range(i + 1, len(source_names)):
                source1 = source_names[i]
                source2 = source_names[j]
                data1 = field_data[source1]
                data2 = field_data[source2]
                
                # è®¡ç®—ç›¸å¯¹å·®å¼‚
                diff = abs(data1 - data2) / (data1 + 1e-10)
                inconsistent = diff > tolerance
                
                if inconsistent.any():
                    inconsistent_indices = diff[inconsistent].index.tolist()
                    result["inconsistent_fields"].append({
                        "field": field,
                        "source1": source1,
                        "source2": source2,
                        "inconsistent_count": len(inconsistent_indices),
                        "inconsistent_indices": inconsistent_indices,
                        "max_diff": float(diff.max())
                    })
    
    result["inconsistency_count"] = len(result["inconsistent_fields"])
    
    # è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†
    total_comparisons = len(common_fields) * len(common_index) * (len(data_sources) - 1) / 2
    if total_comparisons > 0:
        inconsistent_count = sum(v["inconsistent_count"] for v in result["inconsistent_fields"])
        result["consistency_score"] = 1.0 - (inconsistent_count / total_comparisons)
    
    return result
```
-->stent_indices": inconsistent_indices,
                        "max_diff": float(diff.max())
                    })
    
    result["inconsistency_count"] = len(result["inconsistent_fields"])
    
    # è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†
    total_comparisons = len(common_fields) * len(common_index) * (len(data_sources) - 1) / 2
    if total_comp<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_detect_anomalies_statistical.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
import numpy as np
from scipy import stats

def detect_anomalies_statistical(self, data: pd.DataFrame, 
                                fields: List[str] = None,
                                method: str = "zscore",
                                threshold: float = 3.0) -> Dict[str, Any]:
    """
    ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
    
    Args:
        data: å¾…æ£€æµ‹çš„æ•°æ®
        fields: è¦æ£€æµ‹çš„å­—æ®µåˆ—è¡¨
        method: æ£€æµ‹æ–¹æ³•ï¼ˆzscore, iqr, 3sigmaï¼‰
        threshold: é˜ˆå€¼
    
    Returns:
        å¼‚å¸¸æ£€æµ‹ç»“æœ
    """
    result = {
        "anomalies": [],
        "anomaly_count": 0,
        "anomaly_indices": set(),
        "method": method
    }
    
    if fields is None:
        fields = data.select_dtypes(include=[np.number]).columns.tolist()
    
    for field in fields:
        if field not in data.columns:
            continue
        
        values = data[field].dropna()
        
        if method == "zscore":
            # Z-scoreæ–¹æ³•
            z_scores = np.abs(stats.zscore(values))
            anomalies = values[z_scores > threshold]
        
        elif method == "iqr":
            # IQRæ–¹æ³•
            Q1 = values.quantile(0.25)
            Q3 = values.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            anomalies = values[(values < lower_bound) | (values > upper_bound)]
        
        elif method == "3sigma":
            # 3ÏƒåŸåˆ™
            mean = values.mean()
            std = values.std()
            anomalies = values[(values < mean - threshold * std) | 
                              (values > mean + threshold * std)]
        
        if len(anomalies) > 0:
            anomaly_indices = anomalies.index.tolist()
            result["anomalies"].append({
                "field": field,
                "anomaly_count": len(anomalies),
                "anomaly_indices": anomaly_indices,
                "anomaly_values":<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_detect_anomalies_ml.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```
-->gma":
            # 3ÏƒåŸåˆ™
            mean = values.mean()
            std = values.std()
            anomalies = values[(values < mean - threshold * std) | 
                              (values > mean + threshold * std)]
        
        if len(anomalies) > 0:
            anomaly_indices = anomalies.index.tolist()
            result["anomalies"].append({
                "field": field,
                "anomaly_count": len(anomalies),
                "anomaly_indices": anomaly_indices,
                "anomaly_values":<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_detect_anomalies_ml.py"
  language="python<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_clean_missing_values.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def clean_missing_values(self, data: pd.DataFrame,
                       method: str = "forward_fill",
                       **kwargs) -> pd.DataFrame:
    """
    å¤„ç†ç¼ºå¤±å€¼
    
    Args:
        data: å¾…æ¸…æ´—çš„æ•°æ®
        method: å¤„ç†æ–¹æ³•ï¼ˆforward_fill, backward_fill, interpolate, drop, meanï¼‰
        **kwargs: æ–¹æ³•ç‰¹å®šå‚æ•°
    
    Returns:
        æ¸…æ´—åçš„æ•°æ®
    """
    cleaned_data = data.copy()
    
    if method == "forward_fill":
        # å‰å‘å¡«å……
        cleaned_data = cleaned_data.fillna(method='ffill')
    
    elif method == "backward_fill":
        # åå‘å¡«å……
        cleaned_data = cleaned_data.fillna(method='bfill')
    
    elif method == "interpolate":
        # æ’å€¼
        method_type = kwargs.get("interpolation_method", "linear")
        cleaned_data = cleaned_data.interpolate(method=method_type)
    
    elif method == "drop":
        # åˆ é™¤ç¼ºå¤±å€¼
        cleaned_data = cleaned_data.dropna()
    
    elif method == "mean":
        # ç”¨å‡å€¼å¡«å……
        cleaned_data = cleaned_data.fillna(cleaned_data.mean())
    
    elif method == "median":
        # ç”¨ä¸­ä½æ•°å¡«å……
        cleaned_data = cleaned_data.fillna(cleaned_data.median())
    
    return cleaned_data
```
-->forward_fill, backward_fill, interpolate, drop, meanï¼‰
        **kwargs: æ–¹æ³•ç‰¹å®šå‚æ•°
    
    Returns:
        æ¸…æ´—åçš„æ•°æ®
    """
    cleaned_data = data.copy()
    
    if method == "forward_fill":
        # å‰å‘å¡«å……
        cleaned_data = cleaned_data.fillna(method='ffill')
    
    elif method == "backward_fill":
        # åå‘å¡«å……
        cleaned_data = cleaned_data.fillna(method='bfill')
    
    elif method == "interpolate":
        # æ’å€¼
        method_type = kwargs.get("interpolation_method", "linear")
        cleaned_data = cleaned_data.interpolate(method=method_type)
    
    elif method == "drop":
        # åˆ é™¤ç¼ºå¤±å€¼
        cleaned_data = cleaned_data.dropna()
    
    elif method == "mean":
        # ç”¨å‡å€¼å¡«å……
        cleaned_data = cleaned_data.fillna(cleaned_data.mean())
    
    elif method == "median":
        # ç”¨ä¸­ä½æ•°å¡«å……
        cleaned_data = cleaned_data.fillna(cleaned_data.median())
    
    return cleaned_data
```
-->result["anomalies"] = anomaly_data.to_dict('records')
    
    return result
```
-->æµ‹çš„æ•°æ®
        fields: è¦æ£€æµ‹çš„å­—æ®µåˆ—è¡¨
        method: æ£€æµ‹æ–¹æ³•ï¼ˆisolation_forest, dbscanï¼‰
        **kwargs: æ–¹æ³•ç‰¹å®šå‚æ•°
    
    Returns:
        å¼‚å¸¸æ£€æµ‹ç»“æœ
    """
    result = {
        "anomalies": [],
        "anomaly_count": 0,
        "anomaly_indices": [],
        "method": method
    }
    
    if fields is None:
        fields = data.select_<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_clean_anomalies.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def clean_anomalies(self, data: pd.DataFrame,
                   anomaly_indices: List,
                   method: str = "remove",
                   **kwargs) -> pd.DataFrame:
    """
    å¤„ç†å¼‚å¸¸å€¼
    
    Args:
        data: å¾…æ¸…æ´—çš„æ•°æ®
        anomaly_indices: å¼‚å¸¸å€¼ç´¢å¼•åˆ—è¡¨
        method: å¤„ç†æ–¹æ³•ï¼ˆremove, replace, clip, markï¼‰
        **kwargs: æ–¹æ³•ç‰¹å®šå‚æ•°
    
    Returns:
        æ¸…æ´—åçš„æ•°æ®
    """
    cleaned_data = data.copy()
    
    if method == "remove":
        # åˆ é™¤å¼‚å¸¸å€¼
        cleaned_data = cleaned_data.drop(index=anomaly_indices)
    
    elif method == "replace":
        # æ›¿æ¢å¼‚å¸¸å€¼
        replace_value = kwargs.get("replace_value", None)
        if replace_value is None:
            # ç”¨ä¸­ä½æ•°æ›¿æ¢
            for col in cleaned_data.columns:
                if col in cleaned_data.select_dtypes(include=[np.number]).columns:
                    median = cleaned_data[col].median()
                    cleaned_data.loc[anomaly_indices, col] = median
        else:
            cleaned_data.loc[anomaly_indices] = replace_value
    
    elif method == "clip":
        # æˆªæ–­å¼‚å¸¸å€¼
        lower_bound = kwargs.get("lower_bound", None)
        upper_bound = kwargs.get("upper_bound", None)
        for col in cleaned_data.columns:
            if col in cleaned_data.select_dtypes(include=[np.number]).columns:
                if lower_bound is not None:
                    cleaned_data.loc[anomaly_indices, col] = cleaned_data.loc[anomaly_indices, col].clip(lower=lower_bound)
                if upper_bound is not None:
                    cleaned_data.loc[anomaly_indices, col] = cleaned_data.loc[anomaly_indices, col].clip(upper=upper_bound)
    
    elif method<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_auto_clean.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```
-->  if col in cleaned_data.select_dtypes(include=[np.number]).columns:
                    median = cleaned_data[col].median()
                    cleaned_data.loc[anomaly_indices, col] = median
        else:
            cleaned_data.loc[anomaly_indices] = replace_value
    
    elif method == "clip":
        # æˆªæ–­å¼‚å¸¸å€¼
        lower_bound = kwargs.get("lower_bound", None)
        upper_bound = kwargs.get("upper_bound", None)
        for col in cleaned_data.columns:
            if col in cleaned_data.select_dtypes(include=[np.number]).columns:
                if lower_bound is not None:
                    cleaned_data.loc[anomaly_indices, col] = cleaned_data.loc[anomaly_indices, col].clip(lower=lower_bound)
                if upper_bound is not None:
                    cleaned_data.loc[anomaly_indices, col] = cleaned_data.loc[anomaly_indices, col].clip(upper=upper_bound)
    
    elif method<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_auto_clean.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def auto_clean(self, data: pd.DataFrame,
              completeness_threshold: float = 0.95,
              accuracy_threshold: float = 0.99,
              anomaly_method: str = "isolation_forest") -> pd.DataFrame:
    """
    è‡ªåŠ¨æ¸…æ´—æ•°æ®
    
    Args:
<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_generate_quality_report.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def generate_quality_report(self, data: pd.DataFrame,
                           required_fields: List[str] = None) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    
    Args:
        data: å¾…è¯„ä¼°çš„æ•°æ®
        required_fields: å¿…éœ€å­—æ®µåˆ—è¡¨
    
    Returns:
        è´¨é‡æŠ¥å‘Š
    """
    report = {
        "overall_score": 0.0,
        "dimension_scores": {},
        "details": {},
        "recommendations": []
    }
    
    # 1. å®Œæ•´æ€§æ£€æŸ¥
    completeness_result = self.check_completeness(data, required_fields)
    report["dimension_scores"]["completeness"] = completeness_result["completeness_score"]
    report["details"]["completeness"] = completeness_result
    
    # 2. å‡†ç¡®æ€§éªŒè¯
    accuracy_result = self.validate_business_logic(
        data,
        rules={
            "high >= low": "æœ€é«˜ä»·åº”å¤§äºç­‰äºæœ€ä½ä»·",
            "close >= low and close <= high": "æ”¶ç›˜ä»·åº”åœ¨æœ€ä½ä»·å’Œæœ€é«˜ä»·ä¹‹é—´",
            "volume >= 0": "æˆäº¤é‡åº”å¤§äºç­‰äº0"
        }
    )
    report["dimension_scores"]["accuracy"] = accuracy_result["accuracy_score"]
    report["details"]["accuracy"] = accuracy_result
    
    # 3. å¼‚å¸¸æ£€æµ‹
    anomaly_result = self.detect_anomalies(data, methods=["statistical", "isolation_forest"])
    anomaly_rate = anomaly_result["anomaly_count"] / len(data) if len(data) > 0 else 0
    report["dimension_scores"]["anomaly_free"] = 1.0 - anomaly_rate
    report["details"]["anomalies"] = anomaly_result
    
    # 4. è®¡ç®—ç»¼åˆå¾—åˆ†
    weights = {
        "completeness": 0.4,
        "accuracy": 0<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_08.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# åœ¨Cursorä¸­è°ƒç”¨MCPå·¥å…·
# æŸ¥è¯¢æ•°æ®è´¨é‡æ£€æŸ¥ç›¸å…³æ–‡æ¡£
results = kb.query(
    query="æ•°æ®è´¨<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_14.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```
-->completeness"] < 0.95:
            report["recommendations"].append("æ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®")
        if report["dimension_scores"]["accuracy"] < 0.99:
            report["recommendations"].appe<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_monitor_data_quality.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```
-->"æ”¶ç›˜ä»·åº”åœ¨æœ€ä½ä»·å’Œæœ€é«˜ä»·ä¹‹é—´",
            "volume >= 0": "æˆäº¤é‡åº”å¤§äºç­‰äº0"
        }
    )
    report["dimension_scores"]["accuracy"] = accuracy_result["accuracy_score"]
    report["details"]["accuracy"] = accuracy_result
    
    # 3. å¼‚å¸¸æ£€æµ‹
    anomaly_result = self.detect_anomalies(data, methods=["statistical", "isolation_forest"])
    anomaly_rate = anomaly_result["anomaly_count"] / len(data) if len(data) > 0 else 0
    report["dimension_scores"]["anomaly_free"] = 1.0 - anomaly_rate
    report["details"]["anomalies"] = anomaly_result
    
    # 4. è®¡ç®—ç»¼åˆå¾—åˆ†
    weights = {
        "completeness": 0.4,
        "accuracy": 0.4,
        "anomaly_free": 0.2
    }
    report["overall_score"] = sum(
        report["dimension_scores"][dim] * weight
        for dim, weight in weights.items()
    )
    
    # 5. ç”Ÿæˆå»ºè®®
    if report["overall_score"] < 0.9:
        if report["dimension_scores"]["completeness"] < 0.95:
            report["recommendations"].append("æ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®")
        if report["dimension_scores"]["accuracy"] < 0.99:
            report["recommendations"].appe<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_monitor_data_quality.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def monitor_data_quality(self, data_source: str, symbol: str,
                        start_date: str, end_date: str) -> Dict[str, Any]:
    """
    ç›‘æ§æ•°æ®è´¨é‡
    
    Args:
        data_source: æ•°æ®æºåç§°
        symbol: è‚¡ç¥¨ä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        è´¨é‡ç›‘æ§ç»“æœ
    """
    from core.data_source import get_data_source_manager
    
    # è·å–æ•°æ®
    ds_manager = get_data_source_manager()
    data = ds_manager.get_data(symbol, start_date, end_date)
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    report = self.generate_quality_report(data)
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    self._save_quality_report(d<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_13.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# åœ¨Cursorä¸­è°ƒç”¨MCPå·¥å…·
# æŸ¥è¯¢æ•°æ®è´¨é‡æ£€æŸ¥ç›¸å…³æ–‡æ¡£
results = kb.query(
    query="æ•°æ®è´¨<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/002_Chapter2_Data_Source/2.2/code_2_2_14.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# 1. æ”¶é›†å­¦æœ¯è®ºæ–‡
data_collector.collect_academic(
    database="arxiv",
    query="data quality assessment financial time series",
    max_results=10,
    output_dir="data/collected/papers/data_quality"
)

# 2. çˆ¬å–æŠ€æœ¯æ–‡æ¡£
data_col<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_monitor_data_quality.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
def monitor_data_quality(self, data_source: str, symbol: str,
                        start_date: str, end_date: str) -> Dict[str, Any]:
    """
    ç›‘æ§æ•°æ®è´¨é‡
    
    Args:
        data_source: æ•°æ®æºåç§°
        symbol: è‚¡ç¥¨ä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        è´¨é‡ç›‘æ§ç»“æœ
    """
    from core.data_source import get_data_source_manager
    
    # è·å–æ•°æ®
    ds_manager = get_data_source_manager()
    data = ds_manager.get_data(symbol, start_date, end_date)
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    report = self.generate_quality_report(data)
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    self._save_quality_report(data_source, symbol, start_dat<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_10.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# åœ¨Cursorä¸­è°ƒç”¨MCPå·¥å…·
# æŸ¥è¯¢æ•°æ®è´¨é‡æ£€æŸ¥ç›¸å…³æ–‡æ¡£
results = kb.query(
    query="æ•°æ®è´¨é‡æ£€æŸ¥ å¼‚å¸¸æ£€æµ‹ æ•°æ®æ¸…æ´—",
    scope="bo<CodeFromFile 
  filePath="code_library/002_Chapter2_Data_Source/2.2/code_2_2_11.py"
  language="python"
  showDesignPrinciples="true"
/>

<!-- åŸå§‹ä»£ç ï¼ˆä¿ç•™ä½œä¸ºå¤‡ä»½ï¼‰ï¼š
```python
# 1. æ”¶é›†å­¦æœ¯è®ºæ–‡
data_collector.collect_academic(
    database="arxiv",
    query="data quality assessment financial time series",
    max_results=10,
    output_dir="data/collected/papers/data_quality"
)

# 2. çˆ¬å–æŠ€æœ¯æ–‡æ¡£
data_collector.crawl_web(
    url="https://pandas.pydata.org/docs/user_guide/missing_data.html",
    max_depth=1,
    output_dir="data/collected/docs/pandas_missing_data"
)
```
--># 5. ç”Ÿæˆå»ºè®®
    if report["overall_score"] < 0.9:
        if report["dimension_scores"]["completeness"] < 0.95:
            report["recommendations"].append("æ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……ç¼ºå¤±æ•°æ®")
        if report["dimension_scores"]["accuracy"] < 0.99:
            report["recommendations"].append("æ•°æ®å‡†ç¡®æ€§ä¸è¶³ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æº")
        if report["dimension_scores"]["anomaly_free"] < 0.95:
            report["recommendations"].append("æ£€æµ‹åˆ°å¼‚å¸¸å€¼ï¼Œå»ºè®®è¿›è¡Œæ•°æ®æ¸…æ´—")
    
    return report
```

### è´¨é‡ç›‘æ§

ç³»ç»Ÿæ”¯æŒæ•°æ®è´¨é‡ç›‘æ§ï¼Œå®šæœŸç”Ÿæˆè´¨é‡æŠ¥å‘Šï¼š

```python
def monitor_data_quality(self, data_source: str, symbol: str,
                        start_date: str, end_date: str) -> Dict[str, Any]:
    """
    ç›‘æ§æ•°æ®è´¨é‡
    
    Args:
        data_source: æ•°æ®æºåç§°
        symbol: è‚¡ç¥¨ä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
    
    Returns:
        è´¨é‡ç›‘æ§ç»“æœ
    """
    from core.data_source import get_data_source_manager
    
    # è·å–æ•°æ®
    ds_manager = get_data_source_manager()
    data = ds_manager.get_data(symbol, start_date, end_date)
    
    # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    report = self.generate_quality_report(data)
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    self._save_quality_report(data_source, symbol, start_date, end_date, report)
    
    return report
```

<h2 id="section-2-2-6">ğŸ› ï¸ 2.2.6 MCPå·¥å…·ä½¿ç”¨</h2>

åœ¨å¼€å‘æ•°æ®è´¨é‡æ£€æŸ¥åŠŸèƒ½æ—¶ï¼Œå¯ä»¥ä½¿ç”¨MCPå·¥å…·è¿›è¡Œæ·±å…¥ç ”ç©¶å’ŒæŠ€æœ¯è°ƒç ”ã€‚

### æŸ¥è¯¢æ•°æ®è´¨é‡ç›¸å…³æ–‡æ¡£

ä½¿ç”¨`kb.query`æŸ¥è¯¢æ•°æ®è´¨é‡ç›¸å…³çš„æ–‡æ¡£å’Œä»£ç ï¼š

```python
# åœ¨Cursorä¸­è°ƒç”¨MCPå·¥å…·
# æŸ¥è¯¢æ•°æ®è´¨é‡æ£€æŸ¥ç›¸å…³æ–‡æ¡£
results = kb.query(
    query="æ•°æ®è´¨é‡æ£€æŸ¥ å¼‚å¸¸æ£€æµ‹ æ•°æ®æ¸…æ´—",
    scope="both",  # manual + engineering
    top_k=5
)

# æŸ¥è¯¢ç»“æœåŒ…å«ï¼š
# - å¼€å‘æ‰‹å†Œä¸­çš„ç›¸å…³ç« èŠ‚
# - ä»£ç åº“ä¸­çš„æ•°æ®è´¨é‡æ£€æŸ¥å®ç°
# - å¼‚å¸¸æ£€æµ‹ç®—æ³•çš„ä½¿ç”¨ç¤ºä¾‹
```

### æ”¶é›†æ•°æ®è´¨é‡ç ”ç©¶èµ„æ–™

ä½¿ç”¨`data_collector`æ”¶é›†æ•°æ®è´¨é‡ç›¸å…³çš„ç ”ç©¶èµ„æ–™ï¼š

```python
# 1. æ”¶é›†å­¦æœ¯è®ºæ–‡
data_collector.collect_academic(
    database="arxiv",
    query="data quality assessment financial time series",
    max_results=10,
    output_dir="data/collected/papers/data_quality"
)

# 2. çˆ¬å–æŠ€æœ¯æ–‡æ¡£
data_collector.crawl_web(
    url="https://pandas.pydata.org/docs/user_guide/missing_data.html",
    max_depth=1,
    output_dir="data/collected/docs/pandas_missing_data"
)
```

### ä½¿ç”¨MCPå·¥å…·è¿›è¡Œç ”ç©¶

å½“é‡åˆ°æ•°æ®è´¨é‡ç›¸å…³é—®é¢˜æ—¶ï¼Œéµå¾ª"ä¸ç¡®å®šæ—¶ï¼Œå…ˆç ”ç©¶å†å®ç°"çš„å¼€å‘æ–¹æ³•è®ºï¼š

1. **æŸ¥è¯¢çŸ¥è¯†åº“**ï¼šä½¿ç”¨`kb.query`æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£å’Œä»£ç 
2. **æ”¶é›†èµ„æ–™**ï¼šä½¿ç”¨`data_collector`æ”¶é›†å¤–éƒ¨èµ„æ–™
3. **åˆ†æé—®é¢˜**ï¼šç»“åˆæ”¶é›†çš„ä¿¡æ¯åˆ†æé—®é¢˜
4. **å®ç°æ–¹æ¡ˆ**ï¼šåŸºäºç ”ç©¶ç»“æœå®ç°è§£å†³æ–¹æ¡ˆ

è¯¦ç»†æ–¹æ³•è¯·å‚è€ƒ **10.11 å¼€å‘æµç¨‹æ–¹æ³•è®º** ç« èŠ‚ã€‚

## ğŸ”— ç›¸å…³ç« èŠ‚

- **ç¬¬äºŒç«  æ•°æ®æºæ¨¡å—**ï¼šäº†è§£æ•°æ®æºæ¨¡å—çš„æ•´ä½“è®¾è®¡
- **2.1 æ•°æ®æºç®¡ç†**ï¼šäº†è§£æ•°æ®æºç®¡ç†æœºåˆ¶
- **ç¬¬ä¸‰ç«  å¸‚åœºåˆ†æ**ï¼šäº†è§£å¦‚ä½•ä½¿ç”¨é«˜è´¨é‡æ•°æ®è¿›è¡Œå¸‚åœºåˆ†æ
- **10.7 MCP Serverå¼€å‘æŒ‡å—**ï¼šæŒæ¡MCPå·¥å…·å¼€å‘æ–¹æ³•
- **10.11 å¼€å‘æµç¨‹æ–¹æ³•è®º**ï¼šæŒæ¡ä½¿ç”¨MCPå·¥å…·è¿›è¡Œç ”ç©¶çš„æ–¹æ³•

## ğŸ’¡ å…³é”®è¦ç‚¹

1. **å®Œæ•´æ€§æ£€æŸ¥**ï¼šç¼ºå¤±å€¼æ£€æŸ¥ã€æ—¶é—´åºåˆ—è¿ç»­æ€§ã€å­—æ®µå®Œæ•´æ€§
2. **å‡†ç¡®æ€§éªŒè¯**ï¼šæ•°æ®èŒƒå›´æ£€æŸ¥ã€ä¸šåŠ¡é€»è¾‘éªŒè¯ã€å¤šæºæ•°æ®ä¸€è‡´æ€§
3. **å¼‚å¸¸æ£€æµ‹**ï¼šç»Ÿè®¡æ–¹æ³•ã€æœºå™¨å­¦ä¹ æ–¹æ³•ã€æ—¶é—´åºåˆ—æ–¹æ³•
4. **æ•°æ®æ¸…æ´—**ï¼šç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼å¤„ç†ã€æ•°æ®æ ‡å‡†åŒ–
5. **è´¨é‡æŠ¥å‘Š**ï¼šå¤šç»´åº¦è¯„åˆ†ä½“ç³»ã€è´¨é‡ç›‘æ§ã€æ”¹è¿›å»ºè®®
6. **MCPå·¥å…·æ”¯æŒ**ï¼šæ”¯æŒä½¿ç”¨MCPå·¥å…·è¿›è¡Œæ•°æ®è´¨é‡ç›¸å…³ç ”ç©¶

## ğŸ”® æ€»ç»“ä¸å±•æœ›

<div class="summary-outlook">
  <h3>æœ¬èŠ‚å›é¡¾</h3>
  <p>æœ¬èŠ‚ç³»ç»Ÿä»‹ç»äº†æ•°æ®è´¨é‡æ£€æŸ¥æœºåˆ¶ï¼ŒåŒ…æ‹¬æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ã€å‡†ç¡®æ€§éªŒè¯ã€å¼‚å¸¸æ£€æµ‹ä¸æ•°æ®æ¸…æ´—æ–¹æ³•ã€‚é€šè¿‡ç†è§£æ•°æ®è´¨é‡ä¿è¯æœºåˆ¶ï¼Œå¸®åŠ©å¼€å‘è€…ç¡®ä¿åç»­æ­¥éª¤ä½¿ç”¨çš„æ•°æ®è´¨é‡ï¼Œä¸ºæ„å»ºé«˜è´¨é‡çš„æ•°æ®è·å–ç³»ç»Ÿå¥ å®šåŸºç¡€ã€‚</p>
  
  <h3>ä¸‹èŠ‚é¢„å‘Š</h3>
  <p>æŒæ¡äº†æ•°æ®è´¨é‡æ£€æŸ¥åï¼Œä¸‹ä¸€èŠ‚å°†ä»‹ç»æ•°æ®å­˜å‚¨æ¶æ„ï¼ŒåŒ…æ‹¬åˆ†å±‚/å¤šå­˜å‚¨ï¼ˆPolyglot Persistenceï¼‰æ¶æ„è®¾è®¡ã€PostgreSQLã€ClickHouse/TimescaleDBã€Redisç­‰æŠ€æœ¯é€‰å‹ã€‚é€šè¿‡ç†è§£æ•°æ®å­˜å‚¨æ¶æ„ï¼Œå¸®åŠ©å¼€å‘è€…æŒæ¡æ•°æ®å­˜å‚¨çš„æ ¸å¿ƒè®¾è®¡ã€‚</p>
  
  <a href="/ashare-book6/002_Chapter2_Data_Source/2.3_Data_Storage_Architecture_CN" class="next-section">
    ç»§ç»­å­¦ä¹ ï¼š2.3 æ•°æ®å­˜å‚¨æ¶æ„ â†’
  </a>
</div>

> **é€‚ç”¨ç‰ˆæœ¬**: v1.0.0+  
> **æœ€åæ›´æ–°**: 2025-12-12
<!-- Code updated: 2025-12-14T01:30:12.304Z -->
