# æ•°æ®æ”¶é›†å·¥å…·å®‰è£…æŒ‡å—

## ğŸ“¦ å®‰è£…ä¾èµ–

### æ–¹æ³•1: ä½¿ç”¨extensionè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰

```bash
# æ¿€æ´»extensionè™šæ‹Ÿç¯å¢ƒ
source extension/venv/bin/activate  # Linux/macOS
# æˆ–
extension\venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r tools/data_collector/requirements-collector.txt

# å®‰è£…Playwrightæµè§ˆå™¨ï¼ˆå¯é€‰ï¼Œç”¨äºJavaScriptæ¸²æŸ“ï¼‰
playwright install chromium
```

### æ–¹æ³•2: ä½¿ç”¨å®‰è£…è„šæœ¬

```bash
# è¿è¡Œå®‰è£…è„šæœ¬
bash scripts/install_data_collector.sh
```

### æ–¹æ³•3: æ‰‹åŠ¨å®‰è£…

```bash
# æ ¸å¿ƒä¾èµ–
pip install scrapy beautifulsoup4 requests requests-html

# æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆå¯é€‰ï¼‰
pip install playwright selenium
playwright install chromium

# å­¦æœ¯è®ºæ–‡ä¸‹è½½
pip install arxiv feedparser

# PDFå¤„ç†
pip install pypdf2 pdfplumber pymupdf

# æ–‡æœ¬å¤„ç†
pip install markdown html2text

# å·¥å…·
pip install tqdm python-dotenv pyyaml
```

## ğŸ”§ é…ç½®MCPæœåŠ¡å™¨

### 1. æ·»åŠ åˆ° .cursor/mcp.json

å°†ä»¥ä¸‹é…ç½®æ·»åŠ åˆ° `.cursor/mcp.json` çš„ `mcpServers` éƒ¨åˆ†ï¼š

```json
{
  "mcpServers": {
    "data-collector": {
      "command": "python3",
      "args": [
        "mcp_servers/data_collector_server.py"
      ],
      "env": {
        "PYTHONPATH": "${workspaceFolder}"
      }
    }
  }
}
```

### 2. é‡å¯Cursor

é…ç½®å®Œæˆåï¼Œé‡å¯Cursor IDEä»¥ä½¿MCPæœåŠ¡å™¨ç”Ÿæ•ˆã€‚

## ğŸ§ª æµ‹è¯•å®‰è£…

### æµ‹è¯•å·¥å…·

```bash
# è¿è¡Œç¤ºä¾‹ä»£ç 
python tools/data_collector/examples/example_usage.py
```

### æµ‹è¯•MCPæœåŠ¡å™¨

åœ¨Cursorä¸­ï¼Œå¯ä»¥é€šè¿‡MCPå·¥å…·è°ƒç”¨ï¼š
- `data_collector.crawl_web` - çˆ¬å–ç½‘é¡µ
- `data_collector.download_pdf` - ä¸‹è½½PDF
- `data_collector.collect_academic` - æ”¶é›†å­¦æœ¯è®ºæ–‡
- `data_collector.recommend_sources` - æ¨èä¿¡æ¯æº

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Pythonä»£ç ä¸­ä½¿ç”¨

```python
from tools.data_collector import WebCrawler, AcademicScraper

# çˆ¬å–ç½‘é¡µ
crawler = WebCrawler(output_dir="data/collected")
files = crawler.collect("https://example.com", max_depth=2)

# ä¸‹è½½arXivè®ºæ–‡
scraper = AcademicScraper(output_dir="data/papers")
files = scraper.collect("arxiv", "quantitative+trading", max_results=10)
```

### MCPå·¥å…·è°ƒç”¨

åœ¨Cursorä¸­ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨MCPå·¥å…·ï¼š
- å³é”®ç‚¹å‡» â†’ "Call MCP Tool" â†’ é€‰æ‹© `data_collector.crawl_web`
- æˆ–åœ¨å¯¹è¯ä¸­ç›´æ¥ä½¿ç”¨

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **éµå®ˆæ³•å¾‹æ³•è§„**
   - éµå®ˆç½‘ç«™ä½¿ç”¨æ¡æ¬¾
   - å°Šé‡ç‰ˆæƒ
   - ä¸è¦è¿‡åº¦çˆ¬å–

2. **åçˆ¬è™«ç­–ç•¥**
   - ä½¿ç”¨åˆç†çš„è¯·æ±‚é—´éš”
   - ä½¿ç”¨ä»£ç†æ± ï¼ˆå¦‚éœ€è¦ï¼‰
   - ä¼˜å…ˆä½¿ç”¨å®˜æ–¹API

3. **ç½‘ç»œç¯å¢ƒ**
   - æŸäº›ç½‘ç«™å¯èƒ½éœ€è¦ä»£ç†è®¿é—®
   - å­¦æœ¯æ•°æ®åº“å¯èƒ½éœ€è¦æœºæ„è®¢é˜…

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜1: å¯¼å…¥é”™è¯¯

```
ModuleNotFoundError: No module named 'tools'
```

**è§£å†³æ–¹æ¡ˆ**: ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œæˆ–è®¾ç½®PYTHONPATHï¼š
```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### é—®é¢˜2: Playwrightæµè§ˆå™¨æœªå®‰è£…

```
playwright._impl._api_types.Error: Executable doesn't exist
```

**è§£å†³æ–¹æ¡ˆ**: å®‰è£…Playwrightæµè§ˆå™¨ï¼š
```bash
playwright install chromium
```

### é—®é¢˜3: MCPæœåŠ¡å™¨æ— æ³•å¯åŠ¨

**è§£å†³æ–¹æ¡ˆ**: 
1. æ£€æŸ¥Pythonè·¯å¾„æ˜¯å¦æ­£ç¡®
2. æ£€æŸ¥æ–‡ä»¶æƒé™
3. æŸ¥çœ‹é”™è¯¯æ—¥å¿—

---

*æœ€åæ›´æ–°: 2025-12-11*

